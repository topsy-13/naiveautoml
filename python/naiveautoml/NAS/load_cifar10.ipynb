{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of images that are 32x32 pixels in size and have 3 color channels (Red, Green, Blue)  \n",
    "Each image in the CIFAR-10 dataset is represented as a 1D array of length 3072 (32 * 32 * 3).\n",
    "The first 1024 values correspond to the Red channel, the next 1024 values correspond to the Green channel, and the last 1024 values correspond to the Blue channel.\n",
    "\n",
    "The common format for image data in deep learning frameworks like PyTorch is:\n",
    "\n",
    "Shape: (num_samples, channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "20000 20000\n",
      "30000 30000\n",
      "40000 40000\n",
      "50000 50000\n",
      "(50000, 3072) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "X_train, y_train = [], []\n",
    "X_test, y_test = [], []\n",
    "\n",
    "# Load all the paths of the pickle files\n",
    "cifar_path = \"CIFAR-10\"\n",
    "files_path = os.listdir(cifar_path)\n",
    "\n",
    "# Load training data\n",
    "for file in files_path: \n",
    "  filepath = os.path.join(cifar_path, file)\n",
    "  if file.startswith(\"data_batch\"):\n",
    "    temp_dict = unpickle(filepath)\n",
    "    X_train.extend(temp_dict[b'data'])\n",
    "    y_train.extend(temp_dict[b'labels'])\n",
    "    print(len(X_train), len(y_train))\n",
    "\n",
    "# Load testing data\n",
    "for file in files_path:\n",
    "    filepath = os.path.join(cifar_path, file)\n",
    "    if file.startswith(\"test_batch\"):\n",
    "        temp_dict = unpickle(filepath)\n",
    "        X_test.append(temp_dict[b'data'])\n",
    "        y_test.extend(temp_dict[b'labels'])\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Turn into numpy array \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the data\n",
    "# X_train = np.vstack(X_train).reshape(-1, 3, 32, 32) #-1 is the number of samples/images, 3 is the channnels, 32 is the height and 32 is the width\n",
    "X_train = np.vstack(X_train)\n",
    "\n",
    "# X_test = np.vstack(X_test).reshape(-1, 3, 32, 32)\n",
    "X_test= np.vstack(X_test)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 3072), (10000, 3072))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create X_val set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=13)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Select an image index\n",
    "# index = 13\n",
    "\n",
    "# # Get the image\n",
    "# image = X_train[index]\n",
    "\n",
    "# # Plot it\n",
    "# plt.figure(figsize=(1, 1))  \n",
    "# plt.imshow(image.reshape(0, 2, 3, 1))\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PyTorch: Image data in the format (num_samples, channels, height, width) for training and inference.  \n",
    "For Visualization: transpose(1, 2, 0) is required to convert the image to the format (height, width, channels) when using plt.imshow()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = [1, 2, 4]\n",
    "n_neurons_x_layer = [50, 200, 1000]\n",
    "learning_rate = [10**-3, 10**-4, 10**-5]\n",
    "activation = 'relu'\n",
    "solver='adam'\n",
    "\n",
    "# se hacen todas las combinaciones\n",
    "configurations = list(product(n_hidden_layers, n_neurons_x_layer, learning_rate))\n",
    "\n",
    "configuration_results = {\n",
    "    'hidden_layers': [],\n",
    "    'n_neurons': [],\n",
    "    'learning_rate': [],\n",
    "    'train_time': [],\n",
    "    'train_score': [],\n",
    "    'val_score': [],  # stores only the last validation score after each epoch? \n",
    "    'test_score': [],  # stores only the last validation score after each epoch? \n",
    "    'confusion_matrices': [],\n",
    "    'val_accuracy_over_epochs': [] #store learning curves for all configurations\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 27\n",
      "Hidden Layers: 1, # Neurons: 50, Learning rate: 0.001\n",
      "Training Time: 1.2941 seconds\n",
      "Validation Accuracy: 0.1006\n",
      "2 / 27\n",
      "Hidden Layers: 1, # Neurons: 50, Learning rate: 0.0001\n",
      "Training Time: 1.2607 seconds\n",
      "Validation Accuracy: 0.1061\n",
      "3 / 27\n",
      "Hidden Layers: 1, # Neurons: 50, Learning rate: 1e-05\n",
      "Training Time: 1.2636 seconds\n",
      "Validation Accuracy: 0.1170\n",
      "4 / 27\n",
      "Hidden Layers: 1, # Neurons: 200, Learning rate: 0.001\n",
      "Training Time: 3.2648 seconds\n",
      "Validation Accuracy: 0.2283\n",
      "5 / 27\n",
      "Hidden Layers: 1, # Neurons: 200, Learning rate: 0.0001\n",
      "Training Time: 3.2385 seconds\n",
      "Validation Accuracy: 0.2332\n",
      "6 / 27\n",
      "Hidden Layers: 1, # Neurons: 200, Learning rate: 1e-05\n",
      "Training Time: 3.2637 seconds\n",
      "Validation Accuracy: 0.1957\n",
      "7 / 27\n",
      "Hidden Layers: 1, # Neurons: 1000, Learning rate: 0.001\n",
      "Training Time: 14.9456 seconds\n",
      "Validation Accuracy: 0.2136\n",
      "8 / 27\n",
      "Hidden Layers: 1, # Neurons: 1000, Learning rate: 0.0001\n",
      "Training Time: 15.2427 seconds\n",
      "Validation Accuracy: 0.2604\n",
      "9 / 27\n",
      "Hidden Layers: 1, # Neurons: 1000, Learning rate: 1e-05\n",
      "Training Time: 15.3338 seconds\n",
      "Validation Accuracy: 0.2386\n",
      "10 / 27\n",
      "Hidden Layers: 2, # Neurons: 50, Learning rate: 0.001\n",
      "Training Time: 1.3212 seconds\n",
      "Validation Accuracy: 0.0994\n",
      "11 / 27\n",
      "Hidden Layers: 2, # Neurons: 50, Learning rate: 0.0001\n",
      "Training Time: 1.3196 seconds\n",
      "Validation Accuracy: 0.1000\n",
      "12 / 27\n",
      "Hidden Layers: 2, # Neurons: 50, Learning rate: 1e-05\n",
      "Training Time: 1.3105 seconds\n",
      "Validation Accuracy: 0.1365\n",
      "13 / 27\n",
      "Hidden Layers: 2, # Neurons: 200, Learning rate: 0.001\n",
      "Training Time: 3.3568 seconds\n",
      "Validation Accuracy: 0.1978\n",
      "14 / 27\n",
      "Hidden Layers: 2, # Neurons: 200, Learning rate: 0.0001\n",
      "Training Time: 3.4267 seconds\n",
      "Validation Accuracy: 0.2187\n",
      "15 / 27\n",
      "Hidden Layers: 2, # Neurons: 200, Learning rate: 1e-05\n",
      "Training Time: 3.4424 seconds\n",
      "Validation Accuracy: 0.2030\n",
      "16 / 27\n",
      "Hidden Layers: 2, # Neurons: 1000, Learning rate: 0.001\n",
      "Training Time: 20.6383 seconds\n",
      "Validation Accuracy: 0.2462\n",
      "17 / 27\n",
      "Hidden Layers: 2, # Neurons: 1000, Learning rate: 0.0001\n",
      "Training Time: 20.9830 seconds\n",
      "Validation Accuracy: 0.2772\n",
      "18 / 27\n",
      "Hidden Layers: 2, # Neurons: 1000, Learning rate: 1e-05\n",
      "Training Time: 20.5584 seconds\n",
      "Validation Accuracy: 0.2587\n",
      "19 / 27\n",
      "Hidden Layers: 4, # Neurons: 50, Learning rate: 0.001\n",
      "Training Time: 1.3218 seconds\n",
      "Validation Accuracy: 0.2084\n",
      "20 / 27\n",
      "Hidden Layers: 4, # Neurons: 50, Learning rate: 0.0001\n",
      "Training Time: 1.3230 seconds\n",
      "Validation Accuracy: 0.1048\n",
      "21 / 27\n",
      "Hidden Layers: 4, # Neurons: 50, Learning rate: 1e-05\n",
      "Training Time: 1.3237 seconds\n",
      "Validation Accuracy: 0.1106\n",
      "22 / 27\n",
      "Hidden Layers: 4, # Neurons: 200, Learning rate: 0.001\n",
      "Training Time: 3.6299 seconds\n",
      "Validation Accuracy: 0.2661\n",
      "23 / 27\n",
      "Hidden Layers: 4, # Neurons: 200, Learning rate: 0.0001\n",
      "Training Time: 3.6295 seconds\n",
      "Validation Accuracy: 0.2125\n",
      "24 / 27\n",
      "Hidden Layers: 4, # Neurons: 200, Learning rate: 1e-05\n",
      "Training Time: 3.7202 seconds\n",
      "Validation Accuracy: 0.1839\n",
      "25 / 27\n",
      "Hidden Layers: 4, # Neurons: 1000, Learning rate: 0.001\n",
      "Training Time: 30.6186 seconds\n",
      "Validation Accuracy: 0.3471\n",
      "26 / 27\n",
      "Hidden Layers: 4, # Neurons: 1000, Learning rate: 0.0001\n",
      "Training Time: 30.9141 seconds\n",
      "Validation Accuracy: 0.2949\n",
      "27 / 27\n",
      "Hidden Layers: 4, # Neurons: 1000, Learning rate: 1e-05\n",
      "Training Time: 30.9939 seconds\n",
      "Validation Accuracy: 0.2757\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m     configuration_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy_over_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_accuracy_over_epochs)\n\u001b[0;32m     55\u001b[0m configurations_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(configuration_results)\n\u001b[1;32m---> 56\u001b[0m \u001b[43mconfigurations_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResultados por redes neuronales.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\AutoML\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\AutoML\\Lib\\site-packages\\pandas\\core\\generic.py:2417\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2404\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2406\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2407\u001b[0m     df,\n\u001b[0;32m   2408\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2416\u001b[0m )\n\u001b[1;32m-> 2417\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2419\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\AutoML\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\AutoML\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         path,\n\u001b[0;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "for i, (h, n, lr) in enumerate(configurations):\n",
    "    print(i + 1, '/', len(configurations))\n",
    "    print('Hidden Layers: {}, # Neurons: {}, Learning rate: {}'.format(h, n, lr))\n",
    "    # definir estructura de neurona\n",
    "    neuron_structure = (np.ones(h) * n).astype(int)\n",
    "\n",
    "    # Entrenar NN\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(neuron_structure),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate_init=lr\n",
    "    )\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    mlp.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
    "\n",
    "    # Calculate time taken\n",
    "    total_train_time = time.time() - start_time\n",
    "    # Calculate accuracies\n",
    "    train_accuracy = mlp.score(X_train, y_train)\n",
    "    val_accuracy = mlp.score(X_val, y_val)\n",
    "    y_val_pred = mlp.predict(X_val)  # Predict once\n",
    "\n",
    "    # Store validation accuracy\n",
    "    val_accuracy_over_epochs = [accuracy_score(y_val, y_val_pred)]\n",
    "\n",
    "    # Confusion matrix\n",
    "    best_cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = mlp.score(X_test, y_test)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Training Time: {total_train_time:.4f} seconds\")\n",
    "    # print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    # print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    # print(\"Confusion Matrix:\\n\", best_cm)\n",
    "\n",
    "    # Se almacenan los resultados en el dict\n",
    "    configuration_results['hidden_layers'].append(h)\n",
    "    configuration_results['n_neurons'].append(n)\n",
    "    configuration_results['learning_rate'].append(lr)\n",
    "    configuration_results['train_time'].append(total_train_time)\n",
    "    configuration_results['train_score'].append(train_accuracy)\n",
    "    configuration_results['val_score'].append(val_accuracy)\n",
    "    configuration_results['test_score'].append(test_accuracy)\n",
    "    configuration_results['confusion_matrices'].append(best_cm)\n",
    "    configuration_results['val_accuracy_over_epochs'].append(val_accuracy_over_epochs)\n",
    "\n",
    "configurations_df = pd.DataFrame(configuration_results)\n",
    "configurations_df.to_excel('One_Epoch_Results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations_df.to_excel('One_Epoch_Results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hidden_layers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_neurons",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "train_time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "train_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "confusion_matrices",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "val_accuracy_over_epochs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8b711760-ae72-401e-bfd1-c3efa37cab02",
       "rows": [
        [
         "24",
         "4",
         "1000",
         "0.001",
         "30.618587732315063",
         "0.3506",
         "0.3471",
         "0.3467",
         "[[476  25  45  54  33  16  42  15 199  81]\n [ 57 311  19  75  32  30  61  12 117 293]\n [147  24 210 109 157  66 176  34  53  39]\n [ 52  34  58 289  84 165 195  26  51  68]\n [ 94  19 147  90 311  46 218  27  45  49]\n [ 48  26  73 239  81 234 153  24  35  42]\n [ 41  18  85 143 143  47 478   7  27  37]\n [ 54  33  77 130 171  73 130 120  42 113]\n [174  60   9  53  19  25  12   7 512 135]\n [ 54 146  11  46  20  17  47  12 111 530]]",
         "[0.3471]"
        ],
        [
         "25",
         "4",
         "1000",
         "0.0001",
         "30.914071798324585",
         "0.322025",
         "0.2949",
         "0.3119",
         "[[269  67 247  51  30  35  22  89 105  71]\n [ 51 373  77  50  16  42  30  84  63 221]\n [ 46  56 433  63  68  71  98 111  21  48]\n [ 20  63 215 183  31 173 125 123  23  66]\n [ 33  39 378  61 127  72 111 163  22  40]\n [ 13  47 197 180  31 186 118 124  19  40]\n [ 19  42 263  92  49  85 300 104  11  61]\n [ 24  46 182  55  63  62  60 362  24  65]\n [116 132 164  44  19  21  23  65 337  85]\n [ 50 211  67  36  20  38  38 105  50 379]]",
         "[0.2949]"
        ],
        [
         "16",
         "2",
         "1000",
         "0.0001",
         "20.98298192024231",
         "0.28965",
         "0.2772",
         "0.2791",
         "[[407  10  48 150 102  11  13  41 121  83]\n [119 174  30 139  84  21  30  78  97 235]\n [133  15 109 247 290  27  45  71  38  40]\n [ 80  24  56 415 131  63  62  82  39  70]\n [105   7  76 185 408  14  88  90  32  41]\n [ 69  20  78 320 169  79  60  81  32  47]\n [ 58  18  61 317 243  25 160  68  22  54]\n [ 82  14  47 158 227  26  34 249  19  87]\n [221  24  26 134  96  13   6  26 357 103]\n [114  60  23 134  66   9  16  66  92 414]]",
         "[0.2772]"
        ],
        [
         "26",
         "4",
         "1000",
         "1e-05",
         "30.99394679069519",
         "0.2876",
         "0.2757",
         "0.2741",
         "[[404  53  35  61  81  38  16  96 153  49]\n [ 96 312  35  97  63  44  40  85  97 138]\n [126  46 124 130 184 103  97 135  42  28]\n [ 70  75  60 235 103 187  97 113  30  52]\n [115  37  94 104 279 121  89 159  30  18]\n [ 66  45  70 215 100 238  58 114  21  28]\n [ 60  39  77 154 150 117 257 127  14  31]\n [ 87  43  57  96 117 114  61 290  21  57]\n [229  86  30  69  42  34  17  66 367  66]\n [ 87 203  23  94  48  38  41 115  94 251]]",
         "[0.2757]"
        ],
        [
         "21",
         "4",
         "200",
         "0.001",
         "3.6298913955688477",
         "0.2669",
         "0.2661",
         "0.2598",
         "[[246  66 106  35 130  49  26  12 191 125]\n [ 30 385  54  58  79  29  31  18  69 254]\n [ 60  72 190  58 344  73  88  30  55  45]\n [ 30 118 110 133 234 178  93  33  23  70]\n [ 28  66 192  47 453  64  95  34  24  43]\n [ 23  90 101 138 205 230  70  34  22  42]\n [ 12  89 126  90 286 117 219  26  10  51]\n [ 27  66 127  52 264  75  70 127  33 102]\n [108 132  53  42  99  44  11   5 289 223]\n [ 43 273  45  37  72  22  22  25  66 389]]",
         "[0.2661]"
        ],
        [
         "7",
         "1",
         "1000",
         "0.0001",
         "15.24272084236145",
         "0.2691",
         "0.2604",
         "0.2564",
         "[[  0  50 181  43   2   2  18  50 555  85]\n [  1 316  76  50   2   3  52  40 203 264]\n [  1  49 368 111   6  10 120  85 196  69]\n [  0  74 183 286   5  25 116  92 151  90]\n [  1  39 376 118   8   7 134 137 137  89]\n [  0  63 233 257   3  42 108  54 110  85]\n [  0  50 226 193  10  17 266  60 104 100]\n [  0  62 232  99   7  11  63 213 140 116]\n [  1  73  61  37   3   7  15  18 690 101]\n [  1 149  48  45   1   1  27  34 273 415]]",
         "[0.2604]"
        ],
        [
         "17",
         "2",
         "1000",
         "1e-05",
         "20.558359146118164",
         "0.266575",
         "0.2587",
         "0.2577",
         "[[230  58  71  42  52  73  15  68 331  46]\n [ 53 302  31  71  40  65  36  71 166 172]\n [103  44 150  94 162 135  53 116 112  46]\n [ 52  87  86 169 102 203  86  86  84  67]\n [ 64  49 106  89 277 133  85 119  84  40]\n [ 35  58  93 129 116 256  66  97  69  36]\n [ 40  69  89 129 155 143 181  98  71  51]\n [ 61  69  57  80 129 115  47 233  81  71]\n [108  90  38  43  36  51   8  38 515  79]\n [ 52 208  29  59  49  49  22  65 187 274]]",
         "[0.2587]"
        ],
        [
         "15",
         "2",
         "1000",
         "0.001",
         "20.638288259506226",
         "0.24255",
         "0.2462",
         "0.2438",
         "[[370 161  56  40  23  63   4  26 212  31]\n [ 73 510  22  67  23  78  25  32  91  86]\n [142 109 115 193 158 107  43  67  64  17]\n [ 67 189  70 287  52 189  36  60  33  39]\n [112  81  99 250 187 146  60  58  30  23]\n [ 80 156  63 235  51 259  27  52  18  14]\n [ 50 139  89 305  93 163  90  63  12  22]\n [103 168  79 133  98 132  25 115  37  53]\n [220 281  20  20  12  71   3  18 319  42]\n [ 73 400  32  45  12  51  12  50 109 210]]",
         "[0.2462]"
        ],
        [
         "8",
         "1",
         "1000",
         "1e-05",
         "15.333848237991333",
         "0.241875",
         "0.2386",
         "0.2293",
         "[[273  53 100  73  29  46  26  39 228 119]\n [ 86 232  62  90  48  39  56  63  88 243]\n [106  52 237 113 103  95  90  82  58  79]\n [ 72  70 124 197  85 124 111  77  64  98]\n [ 61  51 206 131 213  87 104  79  49  65]\n [ 69  62 154 195  80 155  72  56  59  53]\n [ 37  73 158 175  91  89 212  86  43  62]\n [ 69  68 132  91 112  80  74 161  53 103]\n [124 101  72  55  23  49  20  33 359 170]\n [ 80 158  46  67  35  42  46  67 106 347]]",
         "[0.2386]"
        ],
        [
         "4",
         "1",
         "200",
         "0.0001",
         "3.2384984493255615",
         "0.234675",
         "0.2332",
         "0.2316",
         "[[274  57 188  20  28  44   9  42 144 180]\n [ 59 228 104  46  41  68  37  65  84 275]\n [ 75  42 404  56  82  83  49  80  65  79]\n [ 54  72 272 108  71 137  73  81  67  87]\n [ 41  32 381  53 169 104  53  89  43  81]\n [ 54  53 238  87  68 184  66  91  51  63]\n [ 24  57 284  69  90 138 158  83  46  77]\n [ 68  52 229  49  77  77  60 164  33 134]\n [180  88 105  23  31  41  12  28 243 255]\n [ 45 184  83  20  43  45  24  71  79 400]]",
         "[0.2332]"
        ],
        [
         "3",
         "1",
         "200",
         "0.001",
         "3.264796733856201",
         "0.2198",
         "0.2283",
         "0.2145",
         "[[448  34  11   4 125 112   2   6 244   0]\n [196 332  16   4 103 151   5   5 186   9]\n [196  30  53   6 435 163   7   6 119   0]\n [193  60  26  10 329 250   4  12 137   1]\n [203  21  47   0 559 121   1   7  87   0]\n [133  35  28   5 339 303   4   7  99   2]\n [143  56  70   5 392 203  20   4 133   0]\n [299  23  24   4 286 154   3  25 124   1]\n [213  96   5   2  75  97   0   7 511   0]\n [253 237   7   3  58 113   1   9 291  22]]",
         "[0.2283]"
        ],
        [
         "13",
         "2",
         "200",
         "0.0001",
         "3.42669677734375",
         "0.21925",
         "0.2187",
         "0.2205",
         "[[251  57  95  33 146  57  33  60 180  74]\n [ 73 215  58  57  64  48 144  50 120 178]\n [ 97  73 114  43 244 102 133  94  55  60]\n [ 52 107  58 115 145 168 141 106  56  74]\n [ 95  86  73  55 335 115 139  59  36  53]\n [ 40  87  58 120 165 202  99  78  54  52]\n [ 53 109  67  72 189 142 219  84  44  47]\n [ 85  60  70  54 197  65 101 152  66  93]\n [162  95  66  31  90  57  45  41 320  99]\n [ 90 188  57  33  54  45  90  57 116 264]]",
         "[0.2187]"
        ],
        [
         "6",
         "1",
         "1000",
         "0.001",
         "14.945640802383423",
         "0.211275",
         "0.2136",
         "0.2085",
         "[[154  30   3   9 289  24   4   3 470   0]\n [102 233  15  28 185  82  26  11 316   9]\n [ 54  54  13  23 617  66  15   4 169   0]\n [ 73  82  11  76 466 145  14   4 151   0]\n [ 41  34   8  23 771  50  15   4  99   1]\n [ 43  90   6  87 432 159  12   2 123   1]\n [ 38  77   8  56 629  90  34   5  89   0]\n [ 61  51  10  25 586  64  10  12 123   1]\n [ 95  34   7   9 150  37   2   1 670   1]\n [131 142   6   7 204  46   5  13 426  14]]",
         "[0.2136]"
        ],
        [
         "22",
         "4",
         "200",
         "0.0001",
         "3.6295409202575684",
         "0.2204",
         "0.2125",
         "0.2105",
         "[[240  41  40 108  21 107  25  97 256  51]\n [ 85 225  24  72  43 138  47  80 140 153]\n [ 88  31  55  94  64 324  90 167  68  34]\n [ 59  56  27 188  49 326  78 133  51  55]\n [ 70  37  60  91 131 325 101 148  59  24]\n [ 66  43  38 171  36 332  81 125  29  34]\n [ 39  59  38 120  75 334 164 136  42  19]\n [ 86  47  49  97  77 196  69 203  63  56]\n [186  61  33  58  21  91  26  63 376  91]\n [105 163  18  84  32 107  39  95 140 211]]",
         "[0.2125]"
        ],
        [
         "18",
         "4",
         "50",
         "0.001",
         "1.321775197982788",
         "0.2074",
         "0.2084",
         "0.2012",
         "[[121  26  73  12  15  70  14  64 583   8]\n [ 67 227  65  42  27  73  51  54 343  58]\n [ 30  20 159  72  70 134 109 148 266   7]\n [ 15  39 119 116  80 199  96 102 237  19]\n [ 14  16 165  72  85 128 170 174 214   8]\n [ 16  25 120  92  96 233  68 105 193   7]\n [ 10  23 132 118  98 128 230 133 148   6]\n [ 27  22 180  47  62 115  90 151 233  16]\n [ 78  41  56  20  14  58  15  34 672  18]\n [ 69 183  47  24  10  31  46  79 415  90]]",
         "[0.2084]"
        ],
        [
         "14",
         "2",
         "200",
         "1e-05",
         "3.4424307346343994",
         "0.197725",
         "0.203",
         "0.1982",
         "[[248  86  90  92  53  42  34  58 196  87]\n [ 66 233  91 104  59  83  88  66  78 139]\n [ 98  73 166  91 159  85 113 106  74  50]\n [ 51  82  96 178  84 163 125 120  51  72]\n [ 63  71 145 126 156 102 124 157  46  56]\n [ 37  56 117 134  83 182 124 112  49  61]\n [ 40  70  81 162  93 156 190 146  34  54]\n [ 52  82  98  92 111 101  97 173  37 100]\n [186 101  57 109  49  56  40  47 257 104]\n [ 74 180  77  88  40  83  52  62  91 247]]",
         "[0.203]"
        ],
        [
         "12",
         "2",
         "200",
         "0.001",
         "3.356759786605835",
         "0.195875",
         "0.1978",
         "0.1943",
         "[[ 66   7 308  24 353  63  11  84  58  12]\n [ 18  61 167  94 290  64  28  83  28 174]\n [ 14   3 223  30 569  62  19  66  16  13]\n [  8   6 112  81 466 188  48  76   7  30]\n [ 10   2 122  21 695  74  23  85   2  12]\n [  0   2 122  65 431 214  27  70   9  15]\n [  1   1  71  75 575 139  84  61   4  15]\n [  3   5 141  40 453  71  25 173  12  20]\n [ 62  14 260  32 282  65   5  78 151  57]\n [ 14  49 172  50 255  50  18 121  35 230]]",
         "[0.1978]"
        ],
        [
         "5",
         "1",
         "200",
         "1e-05",
         "3.263684034347534",
         "0.19775",
         "0.1957",
         "0.1997",
         "[[156  80 165  89  82  39  43  72 198  62]\n [ 49 257  51  79  64  88  58  68 114 179]\n [ 74  43 140 169 107 135 113  90  88  56]\n [ 37  77 121 195  93 159 140  86  58  56]\n [ 45  53 132 146 131 155 172  93  75  44]\n [ 37  60 105 198  82 187 105  84  51  46]\n [ 31  66 112 173 122 135 196  85  52  54]\n [ 36  76  91 118 116 127  67 150  80  82]\n [ 95 120  71  63  63  58  37  57 326 116]\n [ 70 171  52  87  56  76  48  79 136 219]]",
         "[0.1957]"
        ],
        [
         "23",
         "4",
         "200",
         "1e-05",
         "3.720227003097534",
         "0.185775",
         "0.1839",
         "0.1888",
         "[[225 120  70  53  84  68  17  95 190  64]\n [ 94 164  45 107  56 110  64  74 157 136]\n [117  82 152 103 137 118  99  74  72  61]\n [ 85  73  96 131 108 188  84 129  66  62]\n [ 85  78 116 105 183 135 131 109  53  51]\n [ 55  76 109 108 114 201  76 106  53  57]\n [ 59 101  95 132 134 136 140 107  40  82]\n [ 75  68  81  91 108 111  81 137  88 103]\n [170 180  44  45  62  49  15  64 294  83]\n [117 136  40  53  65  72  45  99 155 212]]",
         "[0.1839]"
        ],
        [
         "11",
         "2",
         "50",
         "1e-05",
         "1.3105189800262451",
         "0.136375",
         "0.1365",
         "0.135",
         "[[217 115 105  54  68  84  33 147  59 104]\n [ 90 113 133  29  73 147  45  88 108 181]\n [124  73 172  43 143 134  78 131  33  84]\n [ 84 112 132  51  95 182 105 145  27  89]\n [102  86 185  46 134 129  79 155  42  88]\n [ 83  92 132  27  88 161  98 135  34 105]\n [ 55  59 152  51 133 167 131 147  45  86]\n [ 77 105 106  20  74 149  59 154  50 149]\n [235  82 147  31  39  85  31  93  76 187]\n [115 119  75  35  54 178  42 114 106 156]]",
         "[0.1365]"
        ],
        [
         "2",
         "1",
         "50",
         "1e-05",
         "1.263550043106079",
         "0.1151",
         "0.117",
         "0.1115",
         "[[222  62  80  97  87  30  28 178 110  92]\n [ 96 130  88 131 144  77  63  82  83 113]\n [114  66  62  67 136 172  55  90  95 158]\n [112  89  73  71 133 107  87  97 106 147]\n [ 78  69  66  42 107 203  54  91 104 232]\n [110  81  69  65 119 146  72  78  85 130]\n [ 69 102  73  48  90 218  72  89  67 198]\n [107  74  72  34  98  93  76  94 126 169]\n [188  99 136 116  73  24  29 128 132  81]\n [116 109 107  97 122  35  80 115  79 134]]",
         "[0.117]"
        ],
        [
         "20",
         "4",
         "50",
         "1e-05",
         "1.3236699104309082",
         "0.1089",
         "0.1106",
         "0.1111",
         "[[ 31 116   7   1 183 175 173  67 144  89]\n [ 45  69   4   2 114 193 106  88 212 174]\n [ 19 125  10   2 191 175 142  72 118 161]\n [ 21 135   5   3 143 207 163  50 161 134]\n [ 16 108  18   0 179 220 147  59 111 188]\n [ 10 143  13   1 119 233 152  46 135 103]\n [ 11  98  13   0 150 196 226  74 108 150]\n [ 14 106  10   1 115 185 196  97 112 107]\n [ 76 122   5   1 177 164 142  90 144  85]\n [ 59  79   3   1 124 160 137 105 212 114]]",
         "[0.1106]"
        ],
        [
         "1",
         "1",
         "50",
         "0.0001",
         "1.2607462406158447",
         "0.10075",
         "0.1061",
         "0.1013",
         "[[   1    2    4    0  974    0    1    2    1    1]\n [   0    6    8    1  986    0    0    4    1    1]\n [   0    3    6    2  997    1    3    1    1    1]\n [   1    2    4    3 1000    3    3    3    2    1]\n [   0    0    5    0 1037    3    0    1    0    0]\n [   1    1    7    2  933    0    1    7    1    2]\n [   1    2    5    4 1006    1    1    3    1    2]\n [   1    1    2    3  928    1    0    4    0    3]\n [   0    1    2    4  995    1    0    1    2    0]\n [   0    3    5    1  980    2    0    0    2    1]]",
         "[0.1061]"
        ],
        [
         "19",
         "4",
         "50",
         "0.0001",
         "1.323030948638916",
         "0.10445",
         "0.1048",
         "0.1046",
         "[[   6    0    0   16    6    0    2    5  951    0]\n [  11    3    1   51    5    0   21   32  874    9]\n [  10    3    1   25    4    0    4   18  944    6]\n [  12    0    0   38   16    1   10   33  905    7]\n [   2    8    0   20    3    0    1    6 1003    3]\n [  12    6    1   27    6    0   11   21  863    8]\n [   9    4    0   24   10    0    4   19  948    8]\n [   7    2    2   35    6    0   11   22  854    4]\n [   8    4    0   14    6    0    3    9  958    4]\n [  19    1    3   23    9    0    7   14  905   13]]",
         "[0.1048]"
        ],
        [
         "0",
         "1",
         "50",
         "0.001",
         "1.294114351272583",
         "0.099825",
         "0.1006",
         "0.1",
         "[[   0    0    0    0    0    0    0    0  986    0]\n [   0    0    0    0    0    0    0    0 1007    0]\n [   0    0    0    1    0    0    0    0 1014    0]\n [   0    0    0    0    0    0    0    0 1022    0]\n [   0    0    0    0    0    0    0    0 1046    0]\n [   0    0    0    0    0    0    0    0  955    0]\n [   0    0    0    0    0    0    0    0 1026    0]\n [   0    0    0    0    0    0    0    0  943    0]\n [   0    0    0    0    0    0    0    0 1006    0]\n [   0    0    0    0    0    0    0    0  994    0]]",
         "[0.1006]"
        ],
        [
         "10",
         "2",
         "50",
         "0.0001",
         "1.31955885887146",
         "0.100575",
         "0.1",
         "0.1009",
         "[[ 983    0    1    0    0    0    1    0    1    0]\n [ 979    0    7    1    1    1    7    1   10    0]\n [1002    0    7    1    0    0    3    1    1    0]\n [1002    0    7    3    0    0    4    2    4    0]\n [1033    0    4    0    1    1    3    2    2    0]\n [ 939    0    6    0    3    0    4    1    2    0]\n [1003    1    5    2    0    1    5    3    6    0]\n [ 926    0    6    2    1    1    3    0    4    0]\n [ 998    0    5    0    0    0    2    0    1    0]\n [ 984    0    3    0    0    1    2    1    3    0]]",
         "[0.1]"
        ],
        [
         "9",
         "2",
         "50",
         "0.001",
         "1.3211643695831299",
         "0.10015",
         "0.0994",
         "0.1",
         "[[   0    0    0    1    0    0    0    0    0  985]\n [   0    0    0    0    0    0    0    0    0 1007]\n [   0    0    0    0    0    0    0    0    0 1015]\n [   0    0    0    0    0    0    0    0    0 1022]\n [   0    0    0    0    0    0    0    0    0 1046]\n [   0    0    0    0    0    0    0    0    0  955]\n [   0    0    0    0    0    0    0    0    0 1026]\n [   0    0    0    0    0    0    0    0    0  943]\n [   0    0    0    0    0    0    0    0    0 1006]\n [   0    0    0    0    0    0    0    0    0  994]]",
         "[0.0994]"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 27
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>n_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>confusion_matrices</th>\n",
       "      <th>val_accuracy_over_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>30.618588</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.3471</td>\n",
       "      <td>0.3467</td>\n",
       "      <td>[[476, 25, 45, 54, 33, 16, 42, 15, 199, 81], [...</td>\n",
       "      <td>[0.3471]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>30.914072</td>\n",
       "      <td>0.322025</td>\n",
       "      <td>0.2949</td>\n",
       "      <td>0.3119</td>\n",
       "      <td>[[269, 67, 247, 51, 30, 35, 22, 89, 105, 71], ...</td>\n",
       "      <td>[0.2949]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>20.982982</td>\n",
       "      <td>0.289650</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>0.2791</td>\n",
       "      <td>[[407, 10, 48, 150, 102, 11, 13, 41, 121, 83],...</td>\n",
       "      <td>[0.2772]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>30.993947</td>\n",
       "      <td>0.287600</td>\n",
       "      <td>0.2757</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>[[404, 53, 35, 61, 81, 38, 16, 96, 153, 49], [...</td>\n",
       "      <td>[0.2757]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.629891</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>0.2598</td>\n",
       "      <td>[[246, 66, 106, 35, 130, 49, 26, 12, 191, 125]...</td>\n",
       "      <td>[0.2661]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>15.242721</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.2564</td>\n",
       "      <td>[[0, 50, 181, 43, 2, 2, 18, 50, 555, 85], [1, ...</td>\n",
       "      <td>[0.2604]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20.558359</td>\n",
       "      <td>0.266575</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2577</td>\n",
       "      <td>[[230, 58, 71, 42, 52, 73, 15, 68, 331, 46], [...</td>\n",
       "      <td>[0.2587]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>20.638288</td>\n",
       "      <td>0.242550</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.2438</td>\n",
       "      <td>[[370, 161, 56, 40, 23, 63, 4, 26, 212, 31], [...</td>\n",
       "      <td>[0.2462]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>15.333848</td>\n",
       "      <td>0.241875</td>\n",
       "      <td>0.2386</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>[[273, 53, 100, 73, 29, 46, 26, 39, 228, 119],...</td>\n",
       "      <td>[0.2386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.238498</td>\n",
       "      <td>0.234675</td>\n",
       "      <td>0.2332</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>[[274, 57, 188, 20, 28, 44, 9, 42, 144, 180], ...</td>\n",
       "      <td>[0.2332]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.264797</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.2283</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>[[448, 34, 11, 4, 125, 112, 2, 6, 244, 0], [19...</td>\n",
       "      <td>[0.2283]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.426697</td>\n",
       "      <td>0.219250</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>[[251, 57, 95, 33, 146, 57, 33, 60, 180, 74], ...</td>\n",
       "      <td>[0.2187]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>14.945641</td>\n",
       "      <td>0.211275</td>\n",
       "      <td>0.2136</td>\n",
       "      <td>0.2085</td>\n",
       "      <td>[[154, 30, 3, 9, 289, 24, 4, 3, 470, 0], [102,...</td>\n",
       "      <td>[0.2136]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.629541</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>[[240, 41, 40, 108, 21, 107, 25, 97, 256, 51],...</td>\n",
       "      <td>[0.2125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.321775</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.2084</td>\n",
       "      <td>0.2012</td>\n",
       "      <td>[[121, 26, 73, 12, 15, 70, 14, 64, 583, 8], [6...</td>\n",
       "      <td>[0.2084]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3.442431</td>\n",
       "      <td>0.197725</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>[[248, 86, 90, 92, 53, 42, 34, 58, 196, 87], [...</td>\n",
       "      <td>[0.203]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.356760</td>\n",
       "      <td>0.195875</td>\n",
       "      <td>0.1978</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>[[66, 7, 308, 24, 353, 63, 11, 84, 58, 12], [1...</td>\n",
       "      <td>[0.1978]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3.263684</td>\n",
       "      <td>0.197750</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.1997</td>\n",
       "      <td>[[156, 80, 165, 89, 82, 39, 43, 72, 198, 62], ...</td>\n",
       "      <td>[0.1957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3.720227</td>\n",
       "      <td>0.185775</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>[[225, 120, 70, 53, 84, 68, 17, 95, 190, 64], ...</td>\n",
       "      <td>[0.1839]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.310519</td>\n",
       "      <td>0.136375</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>[[217, 115, 105, 54, 68, 84, 33, 147, 59, 104]...</td>\n",
       "      <td>[0.1365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.263550</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>[[222, 62, 80, 97, 87, 30, 28, 178, 110, 92], ...</td>\n",
       "      <td>[0.117]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.323670</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>[[31, 116, 7, 1, 183, 175, 173, 67, 144, 89], ...</td>\n",
       "      <td>[0.1106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.260746</td>\n",
       "      <td>0.100750</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>[[1, 2, 4, 0, 974, 0, 1, 2, 1, 1], [0, 6, 8, 1...</td>\n",
       "      <td>[0.1061]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.323031</td>\n",
       "      <td>0.104450</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>[[6, 0, 0, 16, 6, 0, 2, 5, 951, 0], [11, 3, 1,...</td>\n",
       "      <td>[0.1048]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.294114</td>\n",
       "      <td>0.099825</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 986, 0], [0, 0, 0, 0...</td>\n",
       "      <td>[0.1006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.319559</td>\n",
       "      <td>0.100575</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>[[983, 0, 1, 0, 0, 0, 1, 0, 1, 0], [979, 0, 7,...</td>\n",
       "      <td>[0.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.321164</td>\n",
       "      <td>0.100150</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[[0, 0, 0, 1, 0, 0, 0, 0, 0, 985], [0, 0, 0, 0...</td>\n",
       "      <td>[0.0994]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_layers  n_neurons  learning_rate  train_time  train_score  \\\n",
       "24              4       1000        0.00100   30.618588     0.350600   \n",
       "25              4       1000        0.00010   30.914072     0.322025   \n",
       "16              2       1000        0.00010   20.982982     0.289650   \n",
       "26              4       1000        0.00001   30.993947     0.287600   \n",
       "21              4        200        0.00100    3.629891     0.266900   \n",
       "7               1       1000        0.00010   15.242721     0.269100   \n",
       "17              2       1000        0.00001   20.558359     0.266575   \n",
       "15              2       1000        0.00100   20.638288     0.242550   \n",
       "8               1       1000        0.00001   15.333848     0.241875   \n",
       "4               1        200        0.00010    3.238498     0.234675   \n",
       "3               1        200        0.00100    3.264797     0.219800   \n",
       "13              2        200        0.00010    3.426697     0.219250   \n",
       "6               1       1000        0.00100   14.945641     0.211275   \n",
       "22              4        200        0.00010    3.629541     0.220400   \n",
       "18              4         50        0.00100    1.321775     0.207400   \n",
       "14              2        200        0.00001    3.442431     0.197725   \n",
       "12              2        200        0.00100    3.356760     0.195875   \n",
       "5               1        200        0.00001    3.263684     0.197750   \n",
       "23              4        200        0.00001    3.720227     0.185775   \n",
       "11              2         50        0.00001    1.310519     0.136375   \n",
       "2               1         50        0.00001    1.263550     0.115100   \n",
       "20              4         50        0.00001    1.323670     0.108900   \n",
       "1               1         50        0.00010    1.260746     0.100750   \n",
       "19              4         50        0.00010    1.323031     0.104450   \n",
       "0               1         50        0.00100    1.294114     0.099825   \n",
       "10              2         50        0.00010    1.319559     0.100575   \n",
       "9               2         50        0.00100    1.321164     0.100150   \n",
       "\n",
       "    val_score  test_score                                 confusion_matrices  \\\n",
       "24     0.3471      0.3467  [[476, 25, 45, 54, 33, 16, 42, 15, 199, 81], [...   \n",
       "25     0.2949      0.3119  [[269, 67, 247, 51, 30, 35, 22, 89, 105, 71], ...   \n",
       "16     0.2772      0.2791  [[407, 10, 48, 150, 102, 11, 13, 41, 121, 83],...   \n",
       "26     0.2757      0.2741  [[404, 53, 35, 61, 81, 38, 16, 96, 153, 49], [...   \n",
       "21     0.2661      0.2598  [[246, 66, 106, 35, 130, 49, 26, 12, 191, 125]...   \n",
       "7      0.2604      0.2564  [[0, 50, 181, 43, 2, 2, 18, 50, 555, 85], [1, ...   \n",
       "17     0.2587      0.2577  [[230, 58, 71, 42, 52, 73, 15, 68, 331, 46], [...   \n",
       "15     0.2462      0.2438  [[370, 161, 56, 40, 23, 63, 4, 26, 212, 31], [...   \n",
       "8      0.2386      0.2293  [[273, 53, 100, 73, 29, 46, 26, 39, 228, 119],...   \n",
       "4      0.2332      0.2316  [[274, 57, 188, 20, 28, 44, 9, 42, 144, 180], ...   \n",
       "3      0.2283      0.2145  [[448, 34, 11, 4, 125, 112, 2, 6, 244, 0], [19...   \n",
       "13     0.2187      0.2205  [[251, 57, 95, 33, 146, 57, 33, 60, 180, 74], ...   \n",
       "6      0.2136      0.2085  [[154, 30, 3, 9, 289, 24, 4, 3, 470, 0], [102,...   \n",
       "22     0.2125      0.2105  [[240, 41, 40, 108, 21, 107, 25, 97, 256, 51],...   \n",
       "18     0.2084      0.2012  [[121, 26, 73, 12, 15, 70, 14, 64, 583, 8], [6...   \n",
       "14     0.2030      0.1982  [[248, 86, 90, 92, 53, 42, 34, 58, 196, 87], [...   \n",
       "12     0.1978      0.1943  [[66, 7, 308, 24, 353, 63, 11, 84, 58, 12], [1...   \n",
       "5      0.1957      0.1997  [[156, 80, 165, 89, 82, 39, 43, 72, 198, 62], ...   \n",
       "23     0.1839      0.1888  [[225, 120, 70, 53, 84, 68, 17, 95, 190, 64], ...   \n",
       "11     0.1365      0.1350  [[217, 115, 105, 54, 68, 84, 33, 147, 59, 104]...   \n",
       "2      0.1170      0.1115  [[222, 62, 80, 97, 87, 30, 28, 178, 110, 92], ...   \n",
       "20     0.1106      0.1111  [[31, 116, 7, 1, 183, 175, 173, 67, 144, 89], ...   \n",
       "1      0.1061      0.1013  [[1, 2, 4, 0, 974, 0, 1, 2, 1, 1], [0, 6, 8, 1...   \n",
       "19     0.1048      0.1046  [[6, 0, 0, 16, 6, 0, 2, 5, 951, 0], [11, 3, 1,...   \n",
       "0      0.1006      0.1000  [[0, 0, 0, 0, 0, 0, 0, 0, 986, 0], [0, 0, 0, 0...   \n",
       "10     0.1000      0.1009  [[983, 0, 1, 0, 0, 0, 1, 0, 1, 0], [979, 0, 7,...   \n",
       "9      0.0994      0.1000  [[0, 0, 0, 1, 0, 0, 0, 0, 0, 985], [0, 0, 0, 0...   \n",
       "\n",
       "   val_accuracy_over_epochs  \n",
       "24                 [0.3471]  \n",
       "25                 [0.2949]  \n",
       "16                 [0.2772]  \n",
       "26                 [0.2757]  \n",
       "21                 [0.2661]  \n",
       "7                  [0.2604]  \n",
       "17                 [0.2587]  \n",
       "15                 [0.2462]  \n",
       "8                  [0.2386]  \n",
       "4                  [0.2332]  \n",
       "3                  [0.2283]  \n",
       "13                 [0.2187]  \n",
       "6                  [0.2136]  \n",
       "22                 [0.2125]  \n",
       "18                 [0.2084]  \n",
       "14                  [0.203]  \n",
       "12                 [0.1978]  \n",
       "5                  [0.1957]  \n",
       "23                 [0.1839]  \n",
       "11                 [0.1365]  \n",
       "2                   [0.117]  \n",
       "20                 [0.1106]  \n",
       "1                  [0.1061]  \n",
       "19                 [0.1048]  \n",
       "0                  [0.1006]  \n",
       "10                    [0.1]  \n",
       "9                  [0.0994]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurations_df.sort_values(by='val_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 2.409\n",
      "Epoch 1, Batch 200, Loss: 2.380\n",
      "Epoch 1, Batch 300, Loss: 2.365\n",
      "Epoch 1, Batch 400, Loss: 2.352\n",
      "Epoch 1, Batch 500, Loss: 2.341\n",
      "Epoch 1, Batch 600, Loss: 2.340\n",
      "Epoch 1, Batch 700, Loss: 2.331\n",
      "Epoch 2, Batch 100, Loss: 2.323\n",
      "Epoch 2, Batch 200, Loss: 2.318\n",
      "Epoch 2, Batch 300, Loss: 2.315\n",
      "Epoch 2, Batch 400, Loss: 2.312\n",
      "Epoch 2, Batch 500, Loss: 2.315\n",
      "Epoch 2, Batch 600, Loss: 2.308\n",
      "Epoch 2, Batch 700, Loss: 2.309\n",
      "Epoch 3, Batch 100, Loss: 2.307\n",
      "Epoch 3, Batch 200, Loss: 2.306\n",
      "Epoch 3, Batch 300, Loss: 2.306\n",
      "Epoch 3, Batch 400, Loss: 2.305\n",
      "Epoch 3, Batch 500, Loss: 2.304\n",
      "Epoch 3, Batch 600, Loss: 2.304\n",
      "Epoch 3, Batch 700, Loss: 2.304\n",
      "Epoch 4, Batch 100, Loss: 2.304\n",
      "Epoch 4, Batch 200, Loss: 2.304\n",
      "Epoch 4, Batch 300, Loss: 2.303\n",
      "Epoch 4, Batch 400, Loss: 2.303\n",
      "Epoch 4, Batch 500, Loss: 2.303\n",
      "Epoch 4, Batch 600, Loss: 2.303\n",
      "Epoch 4, Batch 700, Loss: 2.302\n",
      "Epoch 5, Batch 100, Loss: 2.302\n",
      "Epoch 5, Batch 200, Loss: 2.303\n",
      "Epoch 5, Batch 300, Loss: 2.302\n",
      "Epoch 5, Batch 400, Loss: 2.303\n",
      "Epoch 5, Batch 500, Loss: 2.303\n",
      "Epoch 5, Batch 600, Loss: 2.303\n",
      "Epoch 5, Batch 700, Loss: 2.303\n",
      "Finished Training\n",
      "Accuracy of the model on the 10000 test images: 9.97%\n",
      "Predicted labels: [5 5 5 5 5 5 5 5 5 5 5 5 5 8 5 5]\n",
      "True labels: [7 5 8 0 8 2 7 0 3 5 3 8 3 5 1 7]\n"
     ]
    }
   ],
   "source": [
    "class SimpleDenseNN(nn.Module):\n",
    "    def __init__(self, num_layers, neurons_per_layer):\n",
    "        super(SimpleDenseNN, self).__init__()\n",
    "        \n",
    "        # Create a list to hold the layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input size for the first layer\n",
    "        input_size = 3 * 32 * 32  # Assuming input images are flattened (3 channels, 32x32 pixels)\n",
    "        \n",
    "        # Create the specified number of layers\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(input_size, neurons_per_layer))  # Add a dense layer\n",
    "            layers.append(nn.ReLU())  # Add ReLU activation\n",
    "            input_size = neurons_per_layer  # Update input size for the next layer\n",
    "        \n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(neurons_per_layer, 10))  # Assuming 10 output classes for CIFAR-10\n",
    "        \n",
    "        # Combine all layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input tensor\n",
    "        return self.model(x)  # Forward pass through the model\n",
    "\n",
    "# Example usage\n",
    "num_layers = 3  # Desired number of layers\n",
    "neurons_per_layer = 128  # Desired number of neurons per layer\n",
    "model = SimpleDenseNN(num_layers, neurons_per_layer)\n",
    "\n",
    "# Print the model architecture\n",
    "# print(model)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleDenseNN(num_layers=3, neurons_per_layer=2)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()   # Zero the parameter gradients\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Compute loss\n",
    "        loss.backward()         # Backward pass\n",
    "        optimizer.step()        # Optimize the weights\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Save the model (optional)\n",
    "torch.save(model.state_dict(), 'simple_cnn.pth')\n",
    "\n",
    "# Prediction on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # No need to track gradients during evaluation\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n",
    "        total += labels.size(0)  # Update total count\n",
    "        correct += (predicted == labels).sum().item()  # Update correct count\n",
    "\n",
    "print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Print predicted labels\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted labels:', predicted.numpy())\n",
    "print('True labels:', labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of layers and neurons per layer\n",
    "num_layers = 3  # Number of layers\n",
    "neurons_per_layer = 2  # Number of neurons in each layer\n",
    "\n",
    "# Create a list to hold the layers\n",
    "layers = []\n",
    "\n",
    "# Input size for the first layer\n",
    "input_size = 2\n",
    "\n",
    "# Create layers in a loop\n",
    "for _ in range(num_layers):\n",
    "    layer = nn.Linear(input_size, neurons_per_layer)\n",
    "    layers.append(layer)\n",
    "    # Update input_size for the next layer\n",
    "    input_size = neurons_per_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
