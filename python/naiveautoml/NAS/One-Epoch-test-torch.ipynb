{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import load_cifar10\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded succesfully! as <class 'torch.Tensor'>\n",
      "Training data shape: torch.Size([40000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_cifar10.DatasetandLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, neuron_structure, num_classes):\n",
    "        super().__init__()\n",
    "        # Define first layer\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Create hidden layers\n",
    "        # Neuron Structure is expecting a list\n",
    "        for neurons in neuron_structure:\n",
    "            layers.append(nn.Linear(prev_size, neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = neurons\n",
    "\n",
    "        # Define output layer\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 32 * 32 * 3  # 3072 features per image\n",
    "hidden_layers = [512, 256, 128]  # Three hidden layers\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "model = MLP(input_size, hidden_layers, num_classes).to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Flatten images from (batch, 3, 32, 32) to (batch, 3072)\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Flatten images\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return test_loss / len(test_loader), accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] | Train Loss: 1.6965 | Test Loss: 1.5682 | Test Accuracy: 43.78%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1  # Change as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | \"\n",
    "          f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total architectures: 27\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "n_hidden_layers = [1, 2, 4]  # Number of hidden layers\n",
    "n_neurons_x_layer = [50, 200, 1000]  # Neurons per layer\n",
    "learning_rate = [10**-3, 10**-4, 10**-5]  # Learning rates\n",
    "\n",
    "architectures = list(product(n_hidden_layers, n_neurons_x_layer, learning_rate))\n",
    "print('Total architectures:', len(architectures)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing architecture: 1 / 27\n",
      "\n",
      "Training MLP with 1 layers, 50 neurons per layer, LR=0.001\n",
      "Testing architecture: 2 / 27\n",
      "\n",
      "Training MLP with 1 layers, 50 neurons per layer, LR=0.0001\n",
      "Testing architecture: 3 / 27\n",
      "\n",
      "Training MLP with 1 layers, 50 neurons per layer, LR=1e-05\n",
      "Testing architecture: 4 / 27\n",
      "\n",
      "Training MLP with 1 layers, 200 neurons per layer, LR=0.001\n",
      "Testing architecture: 5 / 27\n",
      "\n",
      "Training MLP with 1 layers, 200 neurons per layer, LR=0.0001\n",
      "Testing architecture: 6 / 27\n",
      "\n",
      "Training MLP with 1 layers, 200 neurons per layer, LR=1e-05\n",
      "Testing architecture: 7 / 27\n",
      "\n",
      "Training MLP with 1 layers, 1000 neurons per layer, LR=0.001\n",
      "Testing architecture: 8 / 27\n",
      "\n",
      "Training MLP with 1 layers, 1000 neurons per layer, LR=0.0001\n",
      "Testing architecture: 9 / 27\n",
      "\n",
      "Training MLP with 1 layers, 1000 neurons per layer, LR=1e-05\n",
      "Testing architecture: 10 / 27\n",
      "\n",
      "Training MLP with 2 layers, 50 neurons per layer, LR=0.001\n",
      "Testing architecture: 11 / 27\n",
      "\n",
      "Training MLP with 2 layers, 50 neurons per layer, LR=0.0001\n",
      "Testing architecture: 12 / 27\n",
      "\n",
      "Training MLP with 2 layers, 50 neurons per layer, LR=1e-05\n",
      "Testing architecture: 13 / 27\n",
      "\n",
      "Training MLP with 2 layers, 200 neurons per layer, LR=0.001\n",
      "Testing architecture: 14 / 27\n",
      "\n",
      "Training MLP with 2 layers, 200 neurons per layer, LR=0.0001\n",
      "Testing architecture: 15 / 27\n",
      "\n",
      "Training MLP with 2 layers, 200 neurons per layer, LR=1e-05\n",
      "Testing architecture: 16 / 27\n",
      "\n",
      "Training MLP with 2 layers, 1000 neurons per layer, LR=0.001\n",
      "Testing architecture: 17 / 27\n",
      "\n",
      "Training MLP with 2 layers, 1000 neurons per layer, LR=0.0001\n",
      "Testing architecture: 18 / 27\n",
      "\n",
      "Training MLP with 2 layers, 1000 neurons per layer, LR=1e-05\n",
      "Testing architecture: 19 / 27\n",
      "\n",
      "Training MLP with 4 layers, 50 neurons per layer, LR=0.001\n",
      "Testing architecture: 20 / 27\n",
      "\n",
      "Training MLP with 4 layers, 50 neurons per layer, LR=0.0001\n",
      "Testing architecture: 21 / 27\n",
      "\n",
      "Training MLP with 4 layers, 50 neurons per layer, LR=1e-05\n",
      "Testing architecture: 22 / 27\n",
      "\n",
      "Training MLP with 4 layers, 200 neurons per layer, LR=0.001\n",
      "Testing architecture: 23 / 27\n",
      "\n",
      "Training MLP with 4 layers, 200 neurons per layer, LR=0.0001\n",
      "Testing architecture: 24 / 27\n",
      "\n",
      "Training MLP with 4 layers, 200 neurons per layer, LR=1e-05\n",
      "Testing architecture: 25 / 27\n",
      "\n",
      "Training MLP with 4 layers, 1000 neurons per layer, LR=0.001\n",
      "Testing architecture: 26 / 27\n",
      "\n",
      "Training MLP with 4 layers, 1000 neurons per layer, LR=0.0001\n",
      "Testing architecture: 27 / 27\n",
      "\n",
      "Training MLP with 4 layers, 1000 neurons per layer, LR=1e-05\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define activation functions dynamically\n",
    "activation_functions = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'sigmoid': nn.Sigmoid()\n",
    "}\n",
    "\n",
    "activation = 'relu'\n",
    "\n",
    "# Choose activation function (default to ReLU)\n",
    "activation_fn = activation_functions.get(activation, nn.ReLU())\n",
    "\n",
    "# Loop over all architecture combinations\n",
    "results = []\n",
    "\n",
    "for i, (n_layers, neurons_per_layer, lr) in enumerate(architectures):\n",
    "    print('Testing architecture:', i+1, '/', len(architectures))\n",
    "    print(f\"\\nTraining MLP with {n_layers} layers, {neurons_per_layer} neurons per layer, LR={lr}\")\n",
    "\n",
    "    # Create list of hidden layer sizes\n",
    "    hidden_layers = [neurons_per_layer] * n_layers\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLP(input_size=32*32*3, neuron_structure=hidden_layers, num_classes=10).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 1\n",
    "\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    # print(f\"Epoch | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'n_layers': n_layers,\n",
    "        'neurons_per_layer': neurons_per_layer,\n",
    "        'learning_rate': lr,\n",
    "        'test_accuracy': test_accuracy\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Architecture:\n",
      "{'n_layers': 1, 'neurons_per_layer': 1000, 'learning_rate': 0.0001, 'test_accuracy': 46.99}\n"
     ]
    }
   ],
   "source": [
    "# Sort architectures by highest test accuracy\n",
    "best_architecture = max(results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(\"\\nBest Architecture:\")\n",
    "print(best_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_architecture_results(df, metric):\n",
    "    \"\"\"\n",
    "    Creates unique IDs for neural network architectures and ranks them by a specified metric.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing neural network architecture configurations.\n",
    "        Must have columns 'hidden_layers', 'n_neurons', and 'learning_rate'.\n",
    "    \n",
    "    metric : str\n",
    "        The column name of the metric to use for ranking.\n",
    "        Higher values are assumed to be better.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A copy of the input DataFrame with two new columns:\n",
    "        - 'ID': A string combining the architecture parameters\n",
    "        - 'Ranking': The rank of each architecture based on the metric\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Create an ID for each architecture\n",
    "    result_df['ID'] = (result_df['n_layers'].astype(str) + '_' + \n",
    "                       result_df['neurons_per_layer'].astype(str) + '_' + \n",
    "                       result_df['learning_rate'].astype(str))\n",
    "    \n",
    "    # Sort by metric score\n",
    "    result_df = result_df.sort_values(by=metric, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Add ranking\n",
    "    result_df['Ranking'] = result_df.index + 1\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_layers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "neurons_per_layer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ranking",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c7948cef-3adf-4042-b968-8004d78e46f5",
       "rows": [
        [
         "0",
         "1",
         "1000",
         "0.0001",
         "46.99",
         "1_1000_0.0001",
         "1"
        ],
        [
         "1",
         "4",
         "1000",
         "0.0001",
         "46.56",
         "4_1000_0.0001",
         "2"
        ],
        [
         "2",
         "2",
         "1000",
         "0.0001",
         "46.45",
         "2_1000_0.0001",
         "3"
        ],
        [
         "3",
         "1",
         "200",
         "0.0001",
         "45.13",
         "1_200_0.0001",
         "4"
        ],
        [
         "4",
         "2",
         "200",
         "0.0001",
         "44.7",
         "2_200_0.0001",
         "5"
        ],
        [
         "5",
         "4",
         "200",
         "0.001",
         "44.55",
         "4_200_0.001",
         "6"
        ],
        [
         "6",
         "2",
         "200",
         "0.001",
         "44.52",
         "2_200_0.001",
         "7"
        ],
        [
         "7",
         "2",
         "1000",
         "0.001",
         "44.4",
         "2_1000_0.001",
         "8"
        ],
        [
         "8",
         "1",
         "200",
         "0.001",
         "44.36",
         "1_200_0.001",
         "9"
        ],
        [
         "9",
         "2",
         "50",
         "0.001",
         "44.3",
         "2_50_0.001",
         "10"
        ],
        [
         "10",
         "4",
         "1000",
         "0.001",
         "44.22",
         "4_1000_0.001",
         "11"
        ],
        [
         "11",
         "1",
         "50",
         "0.001",
         "44.03",
         "1_50_0.001",
         "12"
        ],
        [
         "12",
         "4",
         "200",
         "0.0001",
         "43.2",
         "4_200_0.0001",
         "13"
        ],
        [
         "13",
         "4",
         "50",
         "0.001",
         "43.05",
         "4_50_0.001",
         "14"
        ],
        [
         "14",
         "1",
         "50",
         "0.0001",
         "42.5",
         "1_50_0.0001",
         "15"
        ],
        [
         "15",
         "1",
         "1000",
         "0.001",
         "41.69",
         "1_1000_0.001",
         "16"
        ],
        [
         "16",
         "1",
         "1000",
         "1e-05",
         "41.11",
         "1_1000_1e-05",
         "17"
        ],
        [
         "17",
         "2",
         "1000",
         "1e-05",
         "40.41",
         "2_1000_1e-05",
         "18"
        ],
        [
         "18",
         "2",
         "50",
         "0.0001",
         "39.85",
         "2_50_0.0001",
         "19"
        ],
        [
         "19",
         "4",
         "1000",
         "1e-05",
         "38.8",
         "4_1000_1e-05",
         "20"
        ],
        [
         "20",
         "1",
         "200",
         "1e-05",
         "37.22",
         "1_200_1e-05",
         "21"
        ],
        [
         "21",
         "4",
         "50",
         "0.0001",
         "36.91",
         "4_50_0.0001",
         "22"
        ],
        [
         "22",
         "2",
         "200",
         "1e-05",
         "34.24",
         "2_200_1e-05",
         "23"
        ],
        [
         "23",
         "1",
         "50",
         "1e-05",
         "30.87",
         "1_50_1e-05",
         "24"
        ],
        [
         "24",
         "2",
         "50",
         "1e-05",
         "29.35",
         "2_50_1e-05",
         "25"
        ],
        [
         "25",
         "4",
         "200",
         "1e-05",
         "28.52",
         "4_200_1e-05",
         "26"
        ],
        [
         "26",
         "4",
         "50",
         "1e-05",
         "14.16",
         "4_50_1e-05",
         "27"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 27
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>ID</th>\n",
       "      <th>Ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>46.99</td>\n",
       "      <td>1_1000_0.0001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>46.56</td>\n",
       "      <td>4_1000_0.0001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>46.45</td>\n",
       "      <td>2_1000_0.0001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>45.13</td>\n",
       "      <td>1_200_0.0001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>44.70</td>\n",
       "      <td>2_200_0.0001</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>44.55</td>\n",
       "      <td>4_200_0.001</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>44.52</td>\n",
       "      <td>2_200_0.001</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>44.40</td>\n",
       "      <td>2_1000_0.001</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>44.36</td>\n",
       "      <td>1_200_0.001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>44.30</td>\n",
       "      <td>2_50_0.001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>44.22</td>\n",
       "      <td>4_1000_0.001</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>44.03</td>\n",
       "      <td>1_50_0.001</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>43.20</td>\n",
       "      <td>4_200_0.0001</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>43.05</td>\n",
       "      <td>4_50_0.001</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>42.50</td>\n",
       "      <td>1_50_0.0001</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>41.69</td>\n",
       "      <td>1_1000_0.001</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>41.11</td>\n",
       "      <td>1_1000_1e-05</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>40.41</td>\n",
       "      <td>2_1000_1e-05</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>39.85</td>\n",
       "      <td>2_50_0.0001</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>38.80</td>\n",
       "      <td>4_1000_1e-05</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>37.22</td>\n",
       "      <td>1_200_1e-05</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>36.91</td>\n",
       "      <td>4_50_0.0001</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>34.24</td>\n",
       "      <td>2_200_1e-05</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>30.87</td>\n",
       "      <td>1_50_1e-05</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>29.35</td>\n",
       "      <td>2_50_1e-05</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>28.52</td>\n",
       "      <td>4_200_1e-05</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>14.16</td>\n",
       "      <td>4_50_1e-05</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  neurons_per_layer  learning_rate  test_accuracy             ID  \\\n",
       "0          1               1000        0.00010          46.99  1_1000_0.0001   \n",
       "1          4               1000        0.00010          46.56  4_1000_0.0001   \n",
       "2          2               1000        0.00010          46.45  2_1000_0.0001   \n",
       "3          1                200        0.00010          45.13   1_200_0.0001   \n",
       "4          2                200        0.00010          44.70   2_200_0.0001   \n",
       "5          4                200        0.00100          44.55    4_200_0.001   \n",
       "6          2                200        0.00100          44.52    2_200_0.001   \n",
       "7          2               1000        0.00100          44.40   2_1000_0.001   \n",
       "8          1                200        0.00100          44.36    1_200_0.001   \n",
       "9          2                 50        0.00100          44.30     2_50_0.001   \n",
       "10         4               1000        0.00100          44.22   4_1000_0.001   \n",
       "11         1                 50        0.00100          44.03     1_50_0.001   \n",
       "12         4                200        0.00010          43.20   4_200_0.0001   \n",
       "13         4                 50        0.00100          43.05     4_50_0.001   \n",
       "14         1                 50        0.00010          42.50    1_50_0.0001   \n",
       "15         1               1000        0.00100          41.69   1_1000_0.001   \n",
       "16         1               1000        0.00001          41.11   1_1000_1e-05   \n",
       "17         2               1000        0.00001          40.41   2_1000_1e-05   \n",
       "18         2                 50        0.00010          39.85    2_50_0.0001   \n",
       "19         4               1000        0.00001          38.80   4_1000_1e-05   \n",
       "20         1                200        0.00001          37.22    1_200_1e-05   \n",
       "21         4                 50        0.00010          36.91    4_50_0.0001   \n",
       "22         2                200        0.00001          34.24    2_200_1e-05   \n",
       "23         1                 50        0.00001          30.87     1_50_1e-05   \n",
       "24         2                 50        0.00001          29.35     2_50_1e-05   \n",
       "25         4                200        0.00001          28.52    4_200_1e-05   \n",
       "26         4                 50        0.00001          14.16     4_50_1e-05   \n",
       "\n",
       "    Ranking  \n",
       "0         1  \n",
       "1         2  \n",
       "2         3  \n",
       "3         4  \n",
       "4         5  \n",
       "5         6  \n",
       "6         7  \n",
       "7         8  \n",
       "8         9  \n",
       "9        10  \n",
       "10       11  \n",
       "11       12  \n",
       "12       13  \n",
       "13       14  \n",
       "14       15  \n",
       "15       16  \n",
       "16       17  \n",
       "17       18  \n",
       "18       19  \n",
       "19       20  \n",
       "20       21  \n",
       "21       22  \n",
       "22       23  \n",
       "23       24  \n",
       "24       25  \n",
       "25       26  \n",
       "26       27  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Make it a dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = process_architecture_results(results_df, 'test_accuracy')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_layers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "neurons_per_layer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "train_time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "train_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "confusion_matrices",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ranking",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "139aa0ee-9e3e-42e6-86a5-593ee490af55",
       "rows": [
        [
         "0",
         "4",
         "1000",
         "0.0001",
         "30.72800087928772",
         "0.515425",
         "0.4654",
         "0.4739",
         "[[568  31  30  24  11  42  19  50 163  48]\n [ 59 555  11  26  12  12  30  39  95 168]\n [119  26 259  73  99 135 191  66  28  19]\n [ 47  27  64 324  42 236 168  51  30  33]\n [ 82  23 134  63 302  85 206  98  34  19]\n [ 28  23  53 170  34 384 140  71  34  18]\n [ 23  35  67  95  60  86 602  35  11  12]\n [ 50  26  36  56  72  93  59 480  28  43]\n [122  61  11  23  10  28   8  18 653  72]\n [ 55 179   9  33   5  18  23  62  83 527]]",
         "4_1000_0.0001",
         "1"
        ],
        [
         "1",
         "2",
         "1000",
         "0.0001",
         "20.25051116943359",
         "0.5178",
         "0.4628",
         "0.4706",
         "[[524  78  51  47  31  24  15  59 102  55]\n [ 61 648  18  28  20  12  23  37  37 123]\n [ 74  36 311 119 143  80  93 111  21  27]\n [ 22  40  50 445  74 139 102  68  26  56]\n [ 51  37 134  96 383  62  97 128  27  31]\n [ 13  36  61 279  69 299  68  80  24  26]\n [ 13  51  67 193 121  56 453  42  12  18]\n [ 31  35  35  89  76  43  33 520  18  63]\n [130 133  25  41  24  22   6  19 531  75]\n [ 50 269   9  32  16  14  11  41  38 514]]",
         "2_1000_0.0001",
         "2"
        ],
        [
         "2",
         "4",
         "1000",
         "0.001",
         "30.41525936126709",
         "0.4686",
         "0.4498",
         "0.4535",
         "[[543  56  80  24  32  27  36  28  76  84]\n [ 51 595   6  23  15  32  38  22  50 175]\n [ 72  39 364  63 110  95 189  43  13  27]\n [ 28  24  74 253  55 244 234  34  22  54]\n [ 62  26 209  58 288  73 232  58  16  24]\n [ 10  26 100 149  39 417 142  36  16  20]\n [  8  32  99  84  61  84 617  12   7  22]\n [ 31  27  82  38  92  83 101 412  12  65]\n [193  77  33  20  22  47  12  10 467 125]\n [ 51 206   4  24  15  25  53  27  47 542]]",
         "4_1000_0.001",
         "3"
        ],
        [
         "3",
         "1",
         "1000",
         "0.0001",
         "14.9298849105835",
         "0.495275",
         "0.4504",
         "0.4519",
         "[[481  72  79  24  26  34  10  45 125  90]\n [ 50 586  17  39  17  11  16  30  57 184]\n [ 63  59 351 109 128  91  91  69  21  33]\n [ 32  48  82 361  67 177 108  63  26  58]\n [ 50  48 164  79 364  76 106  93  24  42]\n [ 16  53  87 224  67 332  69  49  28  30]\n [ 16  67  99 168 113  73 419  29  12  30]\n [ 31  47  64  80  74  75  25 459  19  69]\n [107  85  32  23  19  38   5  14 565 118]\n [ 32 201   9  38  16  22  10  31  49 586]]",
         "1_1000_0.0001",
         "4"
        ],
        [
         "4",
         "2",
         "200",
         "0.001",
         "3.198516845703125",
         "0.4796",
         "0.443",
         "0.4478",
         "[[444  36  60  25  55  28  29  43 144 122]\n [ 42 505  16  25  19   6  31  29  76 258]\n [ 69  13 252  57 199  94 157 100  28  46]\n [ 19  24  73 230  90 194 207  78  32  75]\n [ 43  22 115  49 403  72 138 121  33  50]\n [ 12  28  62 146  61 340 167  71  27  41]\n [  7  28  76  69 133  46 572  41  15  39]\n [ 24  17  45  34 120  72  50 480  20  81]\n [105  68  17  27  18  21  19  14 548 169]\n [ 34 135   7  15  17  15  28  44  43 656]]",
         "2_200_0.001",
         "5"
        ],
        [
         "5",
         "2",
         "1000",
         "0.001",
         "20.05243682861328",
         "0.48255",
         "0.4511",
         "0.4468",
         "[[538  97  63  15  35  17   3  46 115  57]\n [ 47 686  22  14  12   8  11  27  63 117]\n [104  57 381  70 158  70  52  81  22  20]\n [ 30  68 171 229  88 202  99  67  27  41]\n [ 69  41 219  38 381  71  59 116  23  29]\n [ 29  57 139 136  66 368  56  63  23  18]\n [ 24  68 134 108 157  78 371  53  16  17]\n [ 37  51  70  45  91  70  20 482  15  62]\n [161 107  24  18  14  34   3  12 554  79]\n [ 48 264  18  12  17  15   8  32  59 521]]",
         "2_1000_0.001",
         "6"
        ],
        [
         "6",
         "4",
         "200",
         "0.001",
         "3.545217752456665",
         "0.471625",
         "0.4434",
         "0.4446",
         "[[454  51  82  23  29  27  28  47 222  23]\n [ 42 614  30  22  13  23  26  30 108  99]\n [ 70  26 430  67  79  96 132  67  37  11]\n [ 23  28 140 219  33 286 162  63  52  16]\n [ 54  28 248  38 258  86 176  98  49  11]\n [ 13  36 110 126  30 431 111  60  25  13]\n [ 15  29 146  92  57 107 523  34  16   7]\n [ 37  25  92  47  75  80  56 459  40  32]\n [ 93  78  25  12  13  40   9  22 682  32]\n [ 61 241  19  35   9  25  37  52 151 364]]",
         "4_200_0.001",
         "7"
        ],
        [
         "7",
         "2",
         "200",
         "0.0001",
         "3.232022285461426",
         "0.446875",
         "0.425",
         "0.4272",
         "[[442  73  99  24  14  19  27  47 193  48]\n [ 39 577  26  28  18  14  36  29  88 152]\n [ 86  49 364  83 103  68 124  70  35  33]\n [ 27  56 111 312  56 164 137  59  41  59]\n [ 56  46 214  73 273  58 143 112  39  32]\n [ 19  60 103 210  37 310  99  57  39  21]\n [ 15  61 136 120  83  56 473  38  17  27]\n [ 32  57  82  56  71  73  56 420  31  65]\n [ 96 112  28  32   8  27   5  21 601  76]\n [ 54 234  13  28  12  19  24  37  95 478]]",
         "2_200_0.0001",
         "8"
        ],
        [
         "8",
         "1",
         "1000",
         "0.001",
         "16.62138366699219",
         "0.48675",
         "0.4263",
         "0.426",
         "[[311  51 118  35  39  37  27  48 265  55]\n [ 34 540  10  21  18  20  37  28 148 151]\n [ 33  22 369  71 103 119 160  52  59  27]\n [ 18  24 118 197  51 270 176  54  77  37]\n [ 30  22 191  73 247  90 148 150  69  26]\n [  9  27  84 134  48 383 148  54  38  30]\n [  8  35  81  87  85 126 525  24  35  20]\n [ 22  22  92  50  46  94  65 469  34  49]\n [ 42  64  34  26  17  21  23  12 712  55]\n [ 24 187  27  17  13  30  22  43 121 510]]",
         "1_1000_0.001",
         "9"
        ],
        [
         "9",
         "1",
         "50",
         "0.001",
         "1.178625345230103",
         "0.4512",
         "0.4194",
         "0.4254",
         "[[485  36  58  51  50  24  19  40 166  57]\n [ 43 544  18  40  22  23  41  33  90 153]\n [ 81  26 317 128  85  97 140  91  32  18]\n [ 42  25  91 360  61 178 143  56  35  31]\n [ 57  27 165 103 258  69 204 116  23  24]\n [ 21  34 106 226  55 299 109  57  26  22]\n [ 21  39  82 193  84  78 461  31  15  22]\n [ 47  17  56  85  80  86  52 437  21  62]\n [128  88  27  36  32  25  15  13 570  72]\n [ 50 226  14  45  19  31  25  39  82 463]]",
         "1_50_0.001",
         "10"
        ],
        [
         "10",
         "4",
         "50",
         "0.001",
         "1.17885160446167",
         "0.434325",
         "0.4189",
         "0.4228",
         "[[482  87  72  35  19  12  20  57 134  68]\n [ 49 544  23  40  25  11  29  33  63 190]\n [ 88  23 326 100 147  42 145  91  28  25]\n [ 43  39  85 350  66 130 159  76  29  45]\n [ 75  23 210  80 330  38 137 106  19  28]\n [ 38  32 109 266  64 227  97  75  24  23]\n [ 19  39 111 145 112  47 488  36   8  21]\n [ 60  23  62  72 105  40  62 426  15  78]\n [144 116  35  27  12  33   7  18 513 101]\n [ 42 229  15  44  20   8  26  49  58 503]]",
         "4_50_0.001",
         "11"
        ],
        [
         "11",
         "1",
         "200",
         "0.001",
         "3.097822427749634",
         "0.466475",
         "0.4211",
         "0.4216",
         "[[399  47  61  36  27  31  28 102 182  73]\n [ 67 499  25  28  19  24  17  32 101 195]\n [ 54  25 360 109  82 105 109 106  34  31]\n [ 26  27 112 312  53 198  99  95  53  47]\n [ 37  33 203 102 259  83 108 156  38  27]\n [ 21  21 121 218  30 303  63 109  43  26]\n [ 13  36 155 102  95 128 392  49  26  30]\n [ 50  26  73  71  58 105  22 462  29  47]\n [ 74  57  25  28  10  15  11  36 647 103]\n [ 37 144  23  24  18  25  19  48  78 578]]",
         "1_200_0.001",
         "12"
        ],
        [
         "12",
         "4",
         "200",
         "0.0001",
         "3.54856538772583",
         "0.433875",
         "0.416",
         "0.4212",
         "[[447  57  58  23  16  28  19  54 229  55]\n [ 31 554  21  25  25  35  21  37  90 168]\n [ 96  34 295  53 143 122 123  88  43  18]\n [ 37  51  99 210  53 319  97  64  35  57]\n [ 56  45 152  33 345 117 127 103  38  30]\n [ 20  45 106  98  38 432  84  72  39  21]\n [ 24  56 116  83 106 145 412  40  18  26]\n [ 44  43  65  44 110 111  49 381  34  62]\n [ 86  99  20  15  13  44   5  20 613  91]\n [ 40 241  13  20   9  35  22  42 101 471]]",
         "4_200_0.0001",
         "13"
        ],
        [
         "13",
         "1",
         "200",
         "0.0001",
         "3.073161840438843",
         "0.44255",
         "0.4202",
         "0.4167",
         "[[381  55  84  41  23  24  23  41 225  89]\n [ 19 526  21  32  29  23  41  36  86 194]\n [ 73  40 325  88 138  84 138  61  42  26]\n [ 23  50  86 287  96 143 184  52  45  56]\n [ 40  43 180  71 325  56 166  89  39  37]\n [ 10  40  98 186  88 274 125  63  36  35]\n [ 19  42  85  93 109  52 547  34  12  33]\n [ 26  37  63  60 111  69  78 393  33  73]\n [ 60  88  23  31  19  32  10  21 607 115]\n [ 30 182  16  28  24  25  32  39  81 537]]",
         "1_200_0.0001",
         "14"
        ],
        [
         "14",
         "2",
         "50",
         "0.001",
         "1.112552642822266",
         "0.441325",
         "0.419",
         "0.4149",
         "[[462  57  86  31  19  17  48  37 152  77]\n [ 38 527  27  37  12  17  58  38  73 180]\n [ 72  25 329 105  66  64 239  61  29  25]\n [ 26  38  97 329  41 144 220  50  30  47]\n [ 55  27 179  79 230  45 282  95  27  27]\n [ 24  34 113 226  23 264 161  59  26  25]\n [ 28  22  88 124  61  45 596  30   9  23]\n [ 52  27  86  86  66  48  98 387  25  68]\n [107  91  28  33  14  22  22  16 566 107]\n [ 25 221  21  29  14  17  46  47  74 500]]",
         "2_50_0.001",
         "15"
        ],
        [
         "15",
         "4",
         "1000",
         "1e-05",
         "30.64660286903381",
         "0.386325",
         "0.3747",
         "0.3809",
         "[[423  80  34  22  20  32  18  41 230  86]\n [ 34 467   8  25  32  32  31  46 111 221]\n [106  53 157  69 175 119 117 117  46  56]\n [ 41  62  52 188  89 274  95 105  28  88]\n [ 57  51  86  54 350 115 130 108  41  54]\n [ 28  72  49 117  63 402  65  99  29  31]\n [ 21  96  54 107 127 116 362  86  12  45]\n [ 32  72  25  42 132 113  56 329  34 108]\n [112  93   8  15  17  57   3  23 551 127]\n [ 50 188   5  17  13  23  14  49 117 518]]",
         "4_1000_1e-05",
         "16"
        ],
        [
         "16",
         "2",
         "1000",
         "1e-05",
         "20.18540668487549",
         "0.383525",
         "0.3776",
         "0.3749",
         "[[412  67  64  25  17  31  22  47 225  76]\n [ 46 440  22  36  38  28  43  46  80 228]\n [ 89  42 250  90 139  86 136 104  45  34]\n [ 33  60  75 251  80 227 119  75  37  65]\n [ 51  43 133  61 308  92 141 143  31  43]\n [ 29  49  75 163  55 344  98  79  30  33]\n [ 17  68  85 139 115  85 403  55  17  42]\n [ 35  51  56  61  97 107  62 350  29  95]\n [119 101  17  36  16  46   5  29 534 103]\n [ 48 203  15  22  12  29  33  46 102 484]]",
         "2_1000_1e-05",
         "17"
        ],
        [
         "17",
         "2",
         "50",
         "0.0001",
         "1.123167514801025",
         "0.37135",
         "0.3587",
         "0.3687",
         "[[450  57  10  29  36  28  44  36 206  90]\n [ 56 447   6  46  39  27  53  37  80 216]\n [122  44  43  99 218  58 248 110  37  36]\n [ 56  60  20 221  84 207 196  95  32  51]\n [ 68  38  22  62 378  53 227 118  30  50]\n [ 34  45  13 195  79 309 135  81  33  31]\n [ 27  52  17 127 177  69 450  58  17  32]\n [ 45  39  12  73 159  60  90 340  31  94]\n [168  94   4  35  15  43  15  14 485 133]\n [ 72 200   3  44  16  18  35  46  96 464]]",
         "2_50_0.0001",
         "18"
        ],
        [
         "18",
         "1",
         "50",
         "0.0001",
         "1.06278920173645",
         "0.38475",
         "0.3727",
         "0.3665",
         "[[434  57  42  23  32  47  18  47 205  81]\n [ 47 430  28  34  26  49  46  45  82 220]\n [123  31 195  87 158 128 116  84  47  46]\n [ 58  58  66 218  80 242 122  80  30  68]\n [ 77  42 136  45 309 120 135 106  28  48]\n [ 37  32  54 152  69 370  93  77  33  38]\n [ 28  42  87  88 136 129 406  48  16  46]\n [ 44  42  80  58  69 113  54 366  31  86]\n [129 109  14  23  22  55   8  24 515 107]\n [ 66 188  15  19  12  47  29  47  87 484]]",
         "1_50_0.0001",
         "19"
        ],
        [
         "19",
         "1",
         "1000",
         "1e-05",
         "14.76551985740662",
         "0.369875",
         "0.3585",
         "0.3593",
         "[[460  63  32  50  21  19  24  37 203  77]\n [ 39 438  18  68  35  33  41  33 101 201]\n [107  49 172 124 143  78 163  88  56  35]\n [ 48  71  66 273  86 188 148  50  32  60]\n [ 64  45  98  97 302  75 172 116  40  37]\n [ 29  46  59 191  82 300 107  67  48  26]\n [ 30  63  84 139 123  69 397  65  23  33]\n [ 44  42  44  85 130  75  68 328  43  84]\n [160 110  15  34  23  31  11  25 488 109]\n [ 58 213   9  61  28  24  29  44 101 427]]",
         "1_1000_1e-05",
         "20"
        ],
        [
         "20",
         "4",
         "50",
         "0.0001",
         "1.168257236480713",
         "0.352775",
         "0.3483",
         "0.3515",
         "[[453  57  10  21  27  32  36  34 254  62]\n [ 40 354   4  26  39  32  69  40 108 295]\n [176  36  48  65 159 102 177 145  51  56]\n [ 80  48  22 130  73 271 166 114  27  91]\n [ 97  49  29  47 323  83 181 153  37  47]\n [ 69  47  15  97  51 397 119 100  25  35]\n [ 51  53  40  74 145 111 401 104   7  40]\n [ 60  72  22  44  88  95 110 317  53  82]\n [164  78   6  22  18  41   8  20 523 126]\n [ 44 136   4  18  16  25  52  36 126 537]]",
         "4_50_0.0001",
         "21"
        ],
        [
         "21",
         "2",
         "200",
         "1e-05",
         "3.264208555221558",
         "0.2908",
         "0.2905",
         "0.2996",
         "[[425  90  43  29  28  32  25  42 206  66]\n [ 82 375  33  44  51  35  53  37 116 181]\n [125 115 159  75 147  61 158  74  46  55]\n [ 80 111 105 156 114 153 136  74  33  60]\n [ 73 113  93 103 270  82 160  78  33  41]\n [ 58  95 100 134  84 227  98  66  58  35]\n [ 41 140 117 127 116  90 290  56  18  31]\n [ 66 127  77  81 121  66  66 194  57  88]\n [180 116  16  19  27  62  16  18 454  98]\n [ 84 241  28  23  18  19  30  50 146 355]]",
         "2_200_1e-05",
         "22"
        ],
        [
         "22",
         "1",
         "200",
         "1e-05",
         "3.038786888122559",
         "0.28795",
         "0.2872",
         "0.281",
         "[[380  61  56  34  25  29  40  41 238  82]\n [ 68 301  29  41  55  39  99  39 149 187]\n [117  38 196  88 113  78 210  59  67  49]\n [ 58  56  97 136 102 164 192  83  60  74]\n [ 69  41 123  75 250  78 211  82  64  53]\n [ 60  42  97 140  90 211 143  61  65  46]\n [ 44  33 107  95 128  77 400  45  32  65]\n [ 79  34  75  90 145  63 138 139  70 110]\n [162  86  31  45  26  31  24  11 483 107]\n [ 70 160  29  46  33  14  49  38 179 376]]",
         "1_200_1e-05",
         "23"
        ],
        [
         "23",
         "4",
         "200",
         "1e-05",
         "3.536907196044922",
         "0.269125",
         "0.2732",
         "0.2739",
         "[[461  18  25   6   4  26  68  32 249  97]\n [ 68 163   9  11  11  42 170  55 188 290]\n [205  27 105  11  43  54 234 177  79  80]\n [136  32  59  36  39 122 239 171  87 101]\n [116  22  91  17  85  74 263 238  65  75]\n [128  34  55  31  21 191 237 132  78  48]\n [ 98  37  74  11  36  62 441 161  53  53]\n [ 92  44  43  12  36  58 142 273  87 156]\n [167  30  20   9   3  43  62  17 507 148]\n [ 77  71  12   8   6  17  73  41 219 470]]",
         "4_200_1e-05",
         "24"
        ],
        [
         "24",
         "1",
         "50",
         "1e-05",
         "1.095324993133545",
         "0.2402",
         "0.241",
         "0.2404",
         "[[387  51  88  39  41  53  22  79 129  97]\n [ 99 189  80  75 111  43  51  99  67 193]\n [148  48 141  65 246  73  86 147  20  41]\n [ 70  53 132 146 175 140 100 140  23  43]\n [ 83  48 108  83 311  79 112 154  22  46]\n [ 58  47 113 108 157 189  87 135  29  32]\n [ 51  73 104 100 297  67 202  92   6  34]\n [ 65  66  85  63 168  65  70 243  30  88]\n [199  51  43  29  44  63   8  97 285 187]\n [110 121  74  56  96  39  29  82  70 317]]",
         "1_50_1e-05",
         "25"
        ],
        [
         "25",
         "2",
         "50",
         "1e-05",
         "1.112163066864014",
         "0.217425",
         "0.218",
         "0.222",
         "[[342  51  66  50  50 173  17  18  80 139]\n [ 65 158  60  50 162 172  25  19  56 240]\n [169  25 195 109 232 115  55  38  18  59]\n [ 94  24 114 163 217 223  69  40  20  58]\n [ 87  29 211 116 339  88  84  24  19  49]\n [ 72  33 106 128 194 295  48  21  14  44]\n [ 65  32 193 138 329  94 100  32   3  40]\n [ 67  31 151  85 249 148  59  53  10  90]\n [146  80  26  52  37 212   7   4 189 253]\n [ 79 120  59  52 117 131  28   9  53 346]]",
         "2_50_1e-05",
         "26"
        ],
        [
         "26",
         "4",
         "50",
         "1e-05",
         "1.157088994979858",
         "0.13835",
         "0.1399",
         "0.1402",
         "[[358 173 413   0   0   8  34   0   0   0]\n [139 441 335   0   1   6  85   0   0   0]\n [308 203 443   0   3   6  51   1   0   0]\n [289 210 433   0   0  32  58   0   0   0]\n [270 224 472   0   2  28  42   8   0   0]\n [303 168 399   0   0  38  45   2   0   0]\n [246 246 407   0   0  12 114   1   0   0]\n [201 223 476   0   1  16  23   3   0   0]\n [174 268 509   0   0  14  41   0   0   0]\n [ 98 425 442   0   0   4  25   0   0   0]]",
         "4_50_1e-05",
         "27"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 27
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>confusion_matrices</th>\n",
       "      <th>ID</th>\n",
       "      <th>Ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>30.728001</td>\n",
       "      <td>0.515425</td>\n",
       "      <td>0.4654</td>\n",
       "      <td>0.4739</td>\n",
       "      <td>[[568  31  30  24  11  42  19  50 163  48]\\n [...</td>\n",
       "      <td>4_1000_0.0001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>20.250511</td>\n",
       "      <td>0.517800</td>\n",
       "      <td>0.4628</td>\n",
       "      <td>0.4706</td>\n",
       "      <td>[[524  78  51  47  31  24  15  59 102  55]\\n [...</td>\n",
       "      <td>2_1000_0.0001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>30.415259</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.4498</td>\n",
       "      <td>0.4535</td>\n",
       "      <td>[[543  56  80  24  32  27  36  28  76  84]\\n [...</td>\n",
       "      <td>4_1000_0.001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>14.929885</td>\n",
       "      <td>0.495275</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.4519</td>\n",
       "      <td>[[481  72  79  24  26  34  10  45 125  90]\\n [...</td>\n",
       "      <td>1_1000_0.0001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.198517</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>0.4478</td>\n",
       "      <td>[[444  36  60  25  55  28  29  43 144 122]\\n [...</td>\n",
       "      <td>2_200_0.001</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>20.052437</td>\n",
       "      <td>0.482550</td>\n",
       "      <td>0.4511</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>[[538  97  63  15  35  17   3  46 115  57]\\n [...</td>\n",
       "      <td>2_1000_0.001</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.545218</td>\n",
       "      <td>0.471625</td>\n",
       "      <td>0.4434</td>\n",
       "      <td>0.4446</td>\n",
       "      <td>[[454  51  82  23  29  27  28  47 222  23]\\n [...</td>\n",
       "      <td>4_200_0.001</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.232022</td>\n",
       "      <td>0.446875</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.4272</td>\n",
       "      <td>[[442  73  99  24  14  19  27  47 193  48]\\n [...</td>\n",
       "      <td>2_200_0.0001</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>16.621384</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>0.4263</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>[[311  51 118  35  39  37  27  48 265  55]\\n [...</td>\n",
       "      <td>1_1000_0.001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.178625</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.4194</td>\n",
       "      <td>0.4254</td>\n",
       "      <td>[[485  36  58  51  50  24  19  40 166  57]\\n [...</td>\n",
       "      <td>1_50_0.001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.178852</td>\n",
       "      <td>0.434325</td>\n",
       "      <td>0.4189</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>[[482  87  72  35  19  12  20  57 134  68]\\n [...</td>\n",
       "      <td>4_50_0.001</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.097822</td>\n",
       "      <td>0.466475</td>\n",
       "      <td>0.4211</td>\n",
       "      <td>0.4216</td>\n",
       "      <td>[[399  47  61  36  27  31  28 102 182  73]\\n [...</td>\n",
       "      <td>1_200_0.001</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.548565</td>\n",
       "      <td>0.433875</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>0.4212</td>\n",
       "      <td>[[447  57  58  23  16  28  19  54 229  55]\\n [...</td>\n",
       "      <td>4_200_0.0001</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.073162</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.4202</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>[[381  55  84  41  23  24  23  41 225  89]\\n [...</td>\n",
       "      <td>1_200_0.0001</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.112553</td>\n",
       "      <td>0.441325</td>\n",
       "      <td>0.4190</td>\n",
       "      <td>0.4149</td>\n",
       "      <td>[[462  57  86  31  19  17  48  37 152  77]\\n [...</td>\n",
       "      <td>2_50_0.001</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>30.646603</td>\n",
       "      <td>0.386325</td>\n",
       "      <td>0.3747</td>\n",
       "      <td>0.3809</td>\n",
       "      <td>[[423  80  34  22  20  32  18  41 230  86]\\n [...</td>\n",
       "      <td>4_1000_1e-05</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20.185407</td>\n",
       "      <td>0.383525</td>\n",
       "      <td>0.3776</td>\n",
       "      <td>0.3749</td>\n",
       "      <td>[[412  67  64  25  17  31  22  47 225  76]\\n [...</td>\n",
       "      <td>2_1000_1e-05</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.123168</td>\n",
       "      <td>0.371350</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>0.3687</td>\n",
       "      <td>[[450  57  10  29  36  28  44  36 206  90]\\n [...</td>\n",
       "      <td>2_50_0.0001</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.062789</td>\n",
       "      <td>0.384750</td>\n",
       "      <td>0.3727</td>\n",
       "      <td>0.3665</td>\n",
       "      <td>[[434  57  42  23  32  47  18  47 205  81]\\n [...</td>\n",
       "      <td>1_50_0.0001</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>14.765520</td>\n",
       "      <td>0.369875</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.3593</td>\n",
       "      <td>[[460  63  32  50  21  19  24  37 203  77]\\n [...</td>\n",
       "      <td>1_1000_1e-05</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.168257</td>\n",
       "      <td>0.352775</td>\n",
       "      <td>0.3483</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>[[453  57  10  21  27  32  36  34 254  62]\\n [...</td>\n",
       "      <td>4_50_0.0001</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3.264209</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.2905</td>\n",
       "      <td>0.2996</td>\n",
       "      <td>[[425  90  43  29  28  32  25  42 206  66]\\n [...</td>\n",
       "      <td>2_200_1e-05</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3.038787</td>\n",
       "      <td>0.287950</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>[[380  61  56  34  25  29  40  41 238  82]\\n [...</td>\n",
       "      <td>1_200_1e-05</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3.536907</td>\n",
       "      <td>0.269125</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.2739</td>\n",
       "      <td>[[461  18  25   6   4  26  68  32 249  97]\\n [...</td>\n",
       "      <td>4_200_1e-05</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.095325</td>\n",
       "      <td>0.240200</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.2404</td>\n",
       "      <td>[[387  51  88  39  41  53  22  79 129  97]\\n [...</td>\n",
       "      <td>1_50_1e-05</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.112163</td>\n",
       "      <td>0.217425</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>[[342  51  66  50  50 173  17  18  80 139]\\n [...</td>\n",
       "      <td>2_50_1e-05</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.157089</td>\n",
       "      <td>0.138350</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>[[358 173 413   0   0   8  34   0   0   0]\\n [...</td>\n",
       "      <td>4_50_1e-05</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  neurons_per_layer  learning_rate  train_time  train_score  \\\n",
       "0          4               1000        0.00010   30.728001     0.515425   \n",
       "1          2               1000        0.00010   20.250511     0.517800   \n",
       "2          4               1000        0.00100   30.415259     0.468600   \n",
       "3          1               1000        0.00010   14.929885     0.495275   \n",
       "4          2                200        0.00100    3.198517     0.479600   \n",
       "5          2               1000        0.00100   20.052437     0.482550   \n",
       "6          4                200        0.00100    3.545218     0.471625   \n",
       "7          2                200        0.00010    3.232022     0.446875   \n",
       "8          1               1000        0.00100   16.621384     0.486750   \n",
       "9          1                 50        0.00100    1.178625     0.451200   \n",
       "10         4                 50        0.00100    1.178852     0.434325   \n",
       "11         1                200        0.00100    3.097822     0.466475   \n",
       "12         4                200        0.00010    3.548565     0.433875   \n",
       "13         1                200        0.00010    3.073162     0.442550   \n",
       "14         2                 50        0.00100    1.112553     0.441325   \n",
       "15         4               1000        0.00001   30.646603     0.386325   \n",
       "16         2               1000        0.00001   20.185407     0.383525   \n",
       "17         2                 50        0.00010    1.123168     0.371350   \n",
       "18         1                 50        0.00010    1.062789     0.384750   \n",
       "19         1               1000        0.00001   14.765520     0.369875   \n",
       "20         4                 50        0.00010    1.168257     0.352775   \n",
       "21         2                200        0.00001    3.264209     0.290800   \n",
       "22         1                200        0.00001    3.038787     0.287950   \n",
       "23         4                200        0.00001    3.536907     0.269125   \n",
       "24         1                 50        0.00001    1.095325     0.240200   \n",
       "25         2                 50        0.00001    1.112163     0.217425   \n",
       "26         4                 50        0.00001    1.157089     0.138350   \n",
       "\n",
       "    val_score  test_score                                 confusion_matrices  \\\n",
       "0      0.4654      0.4739  [[568  31  30  24  11  42  19  50 163  48]\\n [...   \n",
       "1      0.4628      0.4706  [[524  78  51  47  31  24  15  59 102  55]\\n [...   \n",
       "2      0.4498      0.4535  [[543  56  80  24  32  27  36  28  76  84]\\n [...   \n",
       "3      0.4504      0.4519  [[481  72  79  24  26  34  10  45 125  90]\\n [...   \n",
       "4      0.4430      0.4478  [[444  36  60  25  55  28  29  43 144 122]\\n [...   \n",
       "5      0.4511      0.4468  [[538  97  63  15  35  17   3  46 115  57]\\n [...   \n",
       "6      0.4434      0.4446  [[454  51  82  23  29  27  28  47 222  23]\\n [...   \n",
       "7      0.4250      0.4272  [[442  73  99  24  14  19  27  47 193  48]\\n [...   \n",
       "8      0.4263      0.4260  [[311  51 118  35  39  37  27  48 265  55]\\n [...   \n",
       "9      0.4194      0.4254  [[485  36  58  51  50  24  19  40 166  57]\\n [...   \n",
       "10     0.4189      0.4228  [[482  87  72  35  19  12  20  57 134  68]\\n [...   \n",
       "11     0.4211      0.4216  [[399  47  61  36  27  31  28 102 182  73]\\n [...   \n",
       "12     0.4160      0.4212  [[447  57  58  23  16  28  19  54 229  55]\\n [...   \n",
       "13     0.4202      0.4167  [[381  55  84  41  23  24  23  41 225  89]\\n [...   \n",
       "14     0.4190      0.4149  [[462  57  86  31  19  17  48  37 152  77]\\n [...   \n",
       "15     0.3747      0.3809  [[423  80  34  22  20  32  18  41 230  86]\\n [...   \n",
       "16     0.3776      0.3749  [[412  67  64  25  17  31  22  47 225  76]\\n [...   \n",
       "17     0.3587      0.3687  [[450  57  10  29  36  28  44  36 206  90]\\n [...   \n",
       "18     0.3727      0.3665  [[434  57  42  23  32  47  18  47 205  81]\\n [...   \n",
       "19     0.3585      0.3593  [[460  63  32  50  21  19  24  37 203  77]\\n [...   \n",
       "20     0.3483      0.3515  [[453  57  10  21  27  32  36  34 254  62]\\n [...   \n",
       "21     0.2905      0.2996  [[425  90  43  29  28  32  25  42 206  66]\\n [...   \n",
       "22     0.2872      0.2810  [[380  61  56  34  25  29  40  41 238  82]\\n [...   \n",
       "23     0.2732      0.2739  [[461  18  25   6   4  26  68  32 249  97]\\n [...   \n",
       "24     0.2410      0.2404  [[387  51  88  39  41  53  22  79 129  97]\\n [...   \n",
       "25     0.2180      0.2220  [[342  51  66  50  50 173  17  18  80 139]\\n [...   \n",
       "26     0.1399      0.1402  [[358 173 413   0   0   8  34   0   0   0]\\n [...   \n",
       "\n",
       "               ID  Ranking  \n",
       "0   4_1000_0.0001        1  \n",
       "1   2_1000_0.0001        2  \n",
       "2    4_1000_0.001        3  \n",
       "3   1_1000_0.0001        4  \n",
       "4     2_200_0.001        5  \n",
       "5    2_1000_0.001        6  \n",
       "6     4_200_0.001        7  \n",
       "7    2_200_0.0001        8  \n",
       "8    1_1000_0.001        9  \n",
       "9      1_50_0.001       10  \n",
       "10     4_50_0.001       11  \n",
       "11    1_200_0.001       12  \n",
       "12   4_200_0.0001       13  \n",
       "13   1_200_0.0001       14  \n",
       "14     2_50_0.001       15  \n",
       "15   4_1000_1e-05       16  \n",
       "16   2_1000_1e-05       17  \n",
       "17    2_50_0.0001       18  \n",
       "18    1_50_0.0001       19  \n",
       "19   1_1000_1e-05       20  \n",
       "20    4_50_0.0001       21  \n",
       "21    2_200_1e-05       22  \n",
       "22    1_200_1e-05       23  \n",
       "23    4_200_1e-05       24  \n",
       "24     1_50_1e-05       25  \n",
       "25     2_50_1e-05       26  \n",
       "26     4_50_1e-05       27  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oe_results = pd.read_excel('One_Epoch_Results.xlsx').rename(columns={'n_neurons': 'neurons_per_layer', \n",
    "                                                                     'hidden_layers': 'n_layers'})\n",
    "oe_results = process_architecture_results(oe_results, 'test_score')\n",
    "oe_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_results = pd.read_excel(\"50 Epochs wo Increase_Results.xlsx\").rename(columns={'n_neurons': 'neurons_per_layer', \n",
    "                                                                     'hidden_layers': 'n_layers'})\n",
    "\n",
    "early_stop_results = process_architecture_results(early_stop_results, 'test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ranking_torch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ranking_one_epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "test_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d1b41b61-a85f-4315-9d8a-f2de93a7c41a",
       "rows": [
        [
         "0",
         "1",
         "4",
         "46.99",
         "0.4519"
        ],
        [
         "1",
         "2",
         "1",
         "46.56",
         "0.4739"
        ],
        [
         "2",
         "3",
         "2",
         "46.45",
         "0.4706"
        ],
        [
         "3",
         "4",
         "14",
         "45.13",
         "0.4167"
        ],
        [
         "4",
         "5",
         "8",
         "44.7",
         "0.4272"
        ],
        [
         "5",
         "6",
         "7",
         "44.55",
         "0.4446"
        ],
        [
         "6",
         "7",
         "5",
         "44.52",
         "0.4478"
        ],
        [
         "7",
         "8",
         "6",
         "44.4",
         "0.4468"
        ],
        [
         "8",
         "9",
         "12",
         "44.36",
         "0.4216"
        ],
        [
         "9",
         "10",
         "15",
         "44.3",
         "0.4149"
        ],
        [
         "10",
         "11",
         "3",
         "44.22",
         "0.4535"
        ],
        [
         "11",
         "12",
         "10",
         "44.03",
         "0.4254"
        ],
        [
         "12",
         "13",
         "13",
         "43.2",
         "0.4212"
        ],
        [
         "13",
         "14",
         "11",
         "43.05",
         "0.4228"
        ],
        [
         "14",
         "15",
         "19",
         "42.5",
         "0.3665"
        ],
        [
         "15",
         "16",
         "9",
         "41.69",
         "0.426"
        ],
        [
         "16",
         "17",
         "20",
         "41.11",
         "0.3593"
        ],
        [
         "17",
         "18",
         "17",
         "40.41",
         "0.3749"
        ],
        [
         "18",
         "19",
         "18",
         "39.85",
         "0.3687"
        ],
        [
         "19",
         "20",
         "16",
         "38.8",
         "0.3809"
        ],
        [
         "20",
         "21",
         "23",
         "37.22",
         "0.281"
        ],
        [
         "21",
         "22",
         "21",
         "36.91",
         "0.3515"
        ],
        [
         "22",
         "23",
         "22",
         "34.24",
         "0.2996"
        ],
        [
         "23",
         "24",
         "25",
         "30.87",
         "0.2404"
        ],
        [
         "24",
         "25",
         "26",
         "29.35",
         "0.222"
        ],
        [
         "25",
         "26",
         "24",
         "28.52",
         "0.2739"
        ],
        [
         "26",
         "27",
         "27",
         "14.16",
         "0.1402"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 27
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking_torch</th>\n",
       "      <th>Ranking_one_epoch</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46.99</td>\n",
       "      <td>0.4519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46.56</td>\n",
       "      <td>0.4739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>46.45</td>\n",
       "      <td>0.4706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>45.13</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>44.70</td>\n",
       "      <td>0.4272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>44.55</td>\n",
       "      <td>0.4446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>44.52</td>\n",
       "      <td>0.4478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>44.40</td>\n",
       "      <td>0.4468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>44.36</td>\n",
       "      <td>0.4216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>44.30</td>\n",
       "      <td>0.4149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>44.22</td>\n",
       "      <td>0.4535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>44.03</td>\n",
       "      <td>0.4254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>43.20</td>\n",
       "      <td>0.4212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>43.05</td>\n",
       "      <td>0.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>42.50</td>\n",
       "      <td>0.3665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>41.69</td>\n",
       "      <td>0.4260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>41.11</td>\n",
       "      <td>0.3593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>40.41</td>\n",
       "      <td>0.3749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>39.85</td>\n",
       "      <td>0.3687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>38.80</td>\n",
       "      <td>0.3809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>37.22</td>\n",
       "      <td>0.2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>36.91</td>\n",
       "      <td>0.3515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>34.24</td>\n",
       "      <td>0.2996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>30.87</td>\n",
       "      <td>0.2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>29.35</td>\n",
       "      <td>0.2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>28.52</td>\n",
       "      <td>0.2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.1402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ranking_torch  Ranking_one_epoch  test_accuracy  test_score\n",
       "0               1                  4          46.99      0.4519\n",
       "1               2                  1          46.56      0.4739\n",
       "2               3                  2          46.45      0.4706\n",
       "3               4                 14          45.13      0.4167\n",
       "4               5                  8          44.70      0.4272\n",
       "5               6                  7          44.55      0.4446\n",
       "6               7                  5          44.52      0.4478\n",
       "7               8                  6          44.40      0.4468\n",
       "8               9                 12          44.36      0.4216\n",
       "9              10                 15          44.30      0.4149\n",
       "10             11                  3          44.22      0.4535\n",
       "11             12                 10          44.03      0.4254\n",
       "12             13                 13          43.20      0.4212\n",
       "13             14                 11          43.05      0.4228\n",
       "14             15                 19          42.50      0.3665\n",
       "15             16                  9          41.69      0.4260\n",
       "16             17                 20          41.11      0.3593\n",
       "17             18                 17          40.41      0.3749\n",
       "18             19                 18          39.85      0.3687\n",
       "19             20                 16          38.80      0.3809\n",
       "20             21                 23          37.22      0.2810\n",
       "21             22                 21          36.91      0.3515\n",
       "22             23                 22          34.24      0.2996\n",
       "23             24                 25          30.87      0.2404\n",
       "24             25                 26          29.35      0.2220\n",
       "25             26                 24          28.52      0.2739\n",
       "26             27                 27          14.16      0.1402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ranking_torch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ranking_early_stop",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "test_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "cfa4baa9-3b6c-41ef-86de-cfe7d5f6a7c3",
       "rows": [
        [
         "0",
         "1",
         "4",
         "46.99",
         "0.481"
        ],
        [
         "1",
         "2",
         "1",
         "46.56",
         "0.492"
        ],
        [
         "2",
         "3",
         "2",
         "46.45",
         "0.4862"
        ],
        [
         "3",
         "4",
         "13",
         "45.13",
         "0.4566"
        ],
        [
         "4",
         "5",
         "17",
         "44.7",
         "0.4458"
        ],
        [
         "5",
         "6",
         "10",
         "44.55",
         "0.4636"
        ],
        [
         "6",
         "7",
         "12",
         "44.52",
         "0.4608"
        ],
        [
         "7",
         "8",
         "11",
         "44.4",
         "0.4618"
        ],
        [
         "8",
         "9",
         "15",
         "44.36",
         "0.4524"
        ],
        [
         "9",
         "10",
         "25",
         "44.3",
         "0.4078"
        ],
        [
         "10",
         "11",
         "14",
         "44.22",
         "0.4564"
        ],
        [
         "11",
         "12",
         "26",
         "44.03",
         "0.4038"
        ],
        [
         "12",
         "13",
         "19",
         "43.2",
         "0.4426"
        ],
        [
         "13",
         "14",
         "27",
         "43.05",
         "0.4016"
        ],
        [
         "14",
         "15",
         "22",
         "42.5",
         "0.4262"
        ],
        [
         "15",
         "16",
         "16",
         "41.69",
         "0.4482"
        ],
        [
         "16",
         "17",
         "3",
         "41.11",
         "0.486"
        ],
        [
         "17",
         "18",
         "6",
         "40.41",
         "0.4762"
        ],
        [
         "18",
         "19",
         "21",
         "39.85",
         "0.4276"
        ],
        [
         "19",
         "20",
         "5",
         "38.8",
         "0.477"
        ],
        [
         "20",
         "21",
         "7",
         "37.22",
         "0.4748"
        ],
        [
         "21",
         "22",
         "24",
         "36.91",
         "0.4188"
        ],
        [
         "22",
         "23",
         "8",
         "34.24",
         "0.4696"
        ],
        [
         "23",
         "24",
         "18",
         "30.87",
         "0.4436"
        ],
        [
         "24",
         "25",
         "23",
         "29.35",
         "0.422"
        ],
        [
         "25",
         "26",
         "9",
         "28.52",
         "0.4652"
        ],
        [
         "26",
         "27",
         "20",
         "14.16",
         "0.4372"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 27
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking_torch</th>\n",
       "      <th>Ranking_early_stop</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46.99</td>\n",
       "      <td>0.4810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46.56</td>\n",
       "      <td>0.4920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>46.45</td>\n",
       "      <td>0.4862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>45.13</td>\n",
       "      <td>0.4566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>44.70</td>\n",
       "      <td>0.4458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>44.55</td>\n",
       "      <td>0.4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>44.52</td>\n",
       "      <td>0.4608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>44.40</td>\n",
       "      <td>0.4618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>44.36</td>\n",
       "      <td>0.4524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>44.30</td>\n",
       "      <td>0.4078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>44.22</td>\n",
       "      <td>0.4564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>44.03</td>\n",
       "      <td>0.4038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>43.20</td>\n",
       "      <td>0.4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>43.05</td>\n",
       "      <td>0.4016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>42.50</td>\n",
       "      <td>0.4262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>41.69</td>\n",
       "      <td>0.4482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>41.11</td>\n",
       "      <td>0.4860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>40.41</td>\n",
       "      <td>0.4762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>39.85</td>\n",
       "      <td>0.4276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>38.80</td>\n",
       "      <td>0.4770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>37.22</td>\n",
       "      <td>0.4748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>36.91</td>\n",
       "      <td>0.4188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>34.24</td>\n",
       "      <td>0.4696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>30.87</td>\n",
       "      <td>0.4436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>29.35</td>\n",
       "      <td>0.4220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>28.52</td>\n",
       "      <td>0.4652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.4372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ranking_torch  Ranking_early_stop  test_accuracy  test_score\n",
       "0               1                   4          46.99      0.4810\n",
       "1               2                   1          46.56      0.4920\n",
       "2               3                   2          46.45      0.4862\n",
       "3               4                  13          45.13      0.4566\n",
       "4               5                  17          44.70      0.4458\n",
       "5               6                  10          44.55      0.4636\n",
       "6               7                  12          44.52      0.4608\n",
       "7               8                  11          44.40      0.4618\n",
       "8               9                  15          44.36      0.4524\n",
       "9              10                  25          44.30      0.4078\n",
       "10             11                  14          44.22      0.4564\n",
       "11             12                  26          44.03      0.4038\n",
       "12             13                  19          43.20      0.4426\n",
       "13             14                  27          43.05      0.4016\n",
       "14             15                  22          42.50      0.4262\n",
       "15             16                  16          41.69      0.4482\n",
       "16             17                   3          41.11      0.4860\n",
       "17             18                   6          40.41      0.4762\n",
       "18             19                  21          39.85      0.4276\n",
       "19             20                   5          38.80      0.4770\n",
       "20             21                   7          37.22      0.4748\n",
       "21             22                  24          36.91      0.4188\n",
       "22             23                   8          34.24      0.4696\n",
       "23             24                  18          30.87      0.4436\n",
       "24             25                  23          29.35      0.4220\n",
       "25             26                   9          28.52      0.4652\n",
       "26             27                  20          14.16      0.4372"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge the two dataframes to check their rankings\n",
    "merged_results = pd.merge(results_df, oe_results, on='ID', suffixes=('_torch', '_one_epoch'))\n",
    "display(merged_results[['Ranking_torch', 'Ranking_one_epoch', 'test_accuracy', 'test_score']])\n",
    "\n",
    "# Merge now early stop results\n",
    "merged_results = pd.merge(results_df, early_stop_results, on='ID', suffixes=('_torch', '_early_stop'))\n",
    "display(merged_results[['Ranking_torch', 'Ranking_early_stop', 'test_accuracy', 'test_score']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
